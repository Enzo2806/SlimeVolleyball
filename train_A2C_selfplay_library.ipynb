{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insipired from: https://github.com/hardmaru/slimevolleygym/blob/master/training_scripts/train_ppo_selfplay.py\n",
    "\n",
    "import os\n",
    "import slimevolleygym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from shutil import copyfile # keep track of generations\n",
    "import torch\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from slimevolleygym import BaselinePolicy\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "NUM_TIMESTEPS = int(5e7)\n",
    "EVAL_EPISODES_SELFPLAY = 100\n",
    "EVAL_EPISODES_BASELINE = 50\n",
    "EVAL_EPISODES_RANDOM = 10\n",
    "BEST_THRESHOLD = 0.5 # must achieve a mean score above this to replace prev best self\n",
    "RENDER_MODE = False # set this to false if you plan on running for full 1000 trials.\n",
    "n_cpu = 25\n",
    "EVAL_FREQ = 250000 // n_cpu\n",
    "learning_rate=0.0007\n",
    "n_steps=5\n",
    "gamma=0.99\n",
    "gae_lambda=1.0\n",
    "ent_coef=0.1\n",
    "vf_coef=0.5\n",
    "max_grad_norm=0.5\n",
    "rms_prop_eps=1e-05\n",
    "use_rms_prop=True\n",
    "use_sde=False\n",
    "sde_sample_freq=-1\n",
    "rollout_buffer_class=None\n",
    "rollout_buffer_kwargs=None\n",
    "normalize_advantage=False\n",
    "stats_window_size=100\n",
    "policy_kwargs=None\n",
    "verbose=1\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "_init_setup_model=True\n",
    "\n",
    "# Log dir\n",
    "LOGDIR = f\"./Logging/A2C-SELFPLAY-LIBRARY/{datetime.now().strftime('%Y%m%d-%H%M%S')}-lr-{learning_rate}-entcoef-{ent_coef}\"\n",
    "os.mkdir(LOGDIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Logging to ./Logging/A2C-SELFPLAY-LIBRARY/20240415-193631-lr-0.0007-entcoef-0.1\\A2C_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 473      |\n",
      "|    ep_rew_mean        | -1.67    |\n",
      "| time/                 |          |\n",
      "|    fps                | 2867     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 12500    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.869    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.103    |\n",
      "|    value_loss         | 0.0321   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 638      |\n",
      "|    ep_rew_mean        | 0.24     |\n",
      "| time/                 |          |\n",
      "|    fps                | 2990     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.924    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.023   |\n",
      "|    value_loss         | 0.0209   |\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 169\u001b[0m\n\u001b[0;32m    166\u001b[0m   model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(LOGDIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 169\u001b[0m   \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[6], line 165\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    153\u001b[0m selfplay_eval_callback \u001b[38;5;241m=\u001b[39m SelfPlayCallback(vec_env,\n\u001b[0;32m    154\u001b[0m                 best_model_save_path\u001b[38;5;241m=\u001b[39mLOGDIR,\n\u001b[0;32m    155\u001b[0m                 log_path\u001b[38;5;241m=\u001b[39mLOGDIR,\n\u001b[0;32m    156\u001b[0m                 eval_freq\u001b[38;5;241m=\u001b[39mEVAL_FREQ,\n\u001b[0;32m    157\u001b[0m                 n_eval_episodes\u001b[38;5;241m=\u001b[39mEVAL_EPISODES_SELFPLAY)\n\u001b[0;32m    159\u001b[0m baseline_eval_callback \u001b[38;5;241m=\u001b[39m BaselineEvalCallback(model, \n\u001b[0;32m    160\u001b[0m                                               EVAL_FREQ, \n\u001b[0;32m    161\u001b[0m                                               EVAL_EPISODES_BASELINE, \n\u001b[0;32m    162\u001b[0m                                               n_cpu, \n\u001b[0;32m    163\u001b[0m                                               selfplay_eval_callback)\n\u001b[1;32m--> 165\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mselfplay_eval_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_eval_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    166\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(LOGDIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\stable_baselines3\\a2c\\a2c.py:201\u001b[0m, in \u001b[0;36mA2C.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[0;32m    194\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    199\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfA2C:\n\u001b[1;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:224\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    221\u001b[0m             terminal_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy\u001b[38;5;241m.\u001b[39mpredict_values(terminal_obs)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    222\u001b[0m         rewards[idx] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma \u001b[38;5;241m*\u001b[39m terminal_value\n\u001b[1;32m--> 224\u001b[0m \u001b[43mrollout_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    225\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_obs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mactions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrewards\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_last_episode_starts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlog_probs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_obs \u001b[38;5;241m=\u001b[39m new_obs  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_last_episode_starts \u001b[38;5;241m=\u001b[39m dones\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\stable_baselines3\\common\\buffers.py:474\u001b[0m, in \u001b[0;36mRolloutBuffer.add\u001b[1;34m(self, obs, action, reward, episode_start, value, log_prob)\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(reward)\n\u001b[0;32m    473\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepisode_starts[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(episode_start)\n\u001b[1;32m--> 474\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalues[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m \u001b[43mvalue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_probs[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos] \u001b[38;5;241m=\u001b[39m log_prob\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpos \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# wrapper over the normal single player env, but loads the best self play model\n",
    "class SlimeVolleySelfPlayEnv(slimevolleygym.SlimeVolleyEnv):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super(SlimeVolleySelfPlayEnv, self).__init__()\n",
    "    self.policy = self\n",
    "    self.best_model = None\n",
    "    self.best_model_filename = None\n",
    "\n",
    "  def predict(self, obs): # the policy\n",
    "    if self.best_model is None:\n",
    "      return self.action_space.sample() # return a random action\n",
    "    else:\n",
    "      action, _ = self.best_model.predict(obs)\n",
    "      return action\n",
    "\n",
    "  # load model if it's there\n",
    "  def reset(self):\n",
    "    modellist = [f for f in os.listdir(LOGDIR) if f.startswith(\"history\")]\n",
    "    modellist.sort()\n",
    "    if len(modellist) > 0:\n",
    "      filename = os.path.join(LOGDIR, modellist[-1]) # the latest best model\n",
    "      if filename != self.best_model_filename:\n",
    "        print(\"loading model: \", filename)\n",
    "        self.best_model_filename = filename\n",
    "        if self.best_model is not None:\n",
    "          del self.best_model\n",
    "        self.best_model = A2C.load(filename, env=self)\n",
    "    return super(SlimeVolleySelfPlayEnv, self).reset()\n",
    "\n",
    "# hacked it to only save new version of best model if beats prev self by BEST_THRESHOLD score\n",
    "# after saving model, resets the best score to be BEST_THRESHOLD\n",
    "class SelfPlayCallback(EvalCallback):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(SelfPlayCallback, self).__init__(*args, **kwargs)\n",
    "    self.best_mean_reward = BEST_THRESHOLD\n",
    "    self.generation = 0\n",
    "  def _on_step(self) -> bool:\n",
    "    result = super(SelfPlayCallback, self)._on_step()\n",
    "    if result and self.best_mean_reward > BEST_THRESHOLD:\n",
    "      self.generation += 1\n",
    "      print(\"SELFPLAY: mean_reward achieved:\", self.best_mean_reward)\n",
    "      print(\"SELFPLAY: new best model, bumping up generation to\", self.generation)\n",
    "      source_file = os.path.join(LOGDIR, \"best_model.zip\")\n",
    "      backup_file = os.path.join(LOGDIR, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n",
    "      copyfile(source_file, backup_file)\n",
    "      self.best_mean_reward = BEST_THRESHOLD\n",
    "    return result\n",
    "\n",
    "class BaselineEvalCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    Used to evaluate the agent against the baseline every certain number of iterations\n",
    "\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, model, eval_freq, num_evals, n_cpu, selfplaycallback, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        \n",
    "        # Store the instance variables\n",
    "        self.model = model\n",
    "        self.eval_freq = eval_freq\n",
    "        self.num_evals = num_evals\n",
    "        self.baseline = BaselinePolicy()\n",
    "        self.last_generation = 0\n",
    "        self.selfplaycallback = selfplaycallback # Store the selfplay callback to access the generation number\n",
    "\n",
    "        # Make the regular environment with the opponent being the baseline instead of selfplay for this callback\n",
    "        self.vecenv = make_vec_env(slimevolleygym.SlimeVolleyEnv, n_envs=n_cpu, seed=SEED)\n",
    "\n",
    "        # Create a summarywriter at the logdir\n",
    "        self.writer = SummaryWriter(log_dir=LOGDIR)\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.n_calls > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            \n",
    "            # Evaluate the model in the vectorized environment\n",
    "            mean_reward, std_reward = evaluate_policy(self.model, self.vecenv, n_eval_episodes=self.num_evals)\n",
    "\n",
    "            # Log the mean and std reward\n",
    "            self.writer.add_scalar(\"Average baseline test return - Training step\", mean_reward, self.n_calls)\n",
    "            self.writer.add_scalar(\"Baseline test return standard deviation - Training step\", std_reward, self.n_calls)\n",
    "\n",
    "            # If the generation number increased\n",
    "            if self.selfplaycallback.generation > self.last_generation:\n",
    "                \n",
    "                # Update the generation number to match\n",
    "                self.last_generation = self.selfplaycallback.generation\n",
    "               \n",
    "                # Log the mean and std reward\n",
    "                self.writer.add_scalar(\"Average baseline test return - Generation\", mean_reward, self.last_generation)\n",
    "                self.writer.add_scalar(\"Baseline test return standard deviation - Generation\", mean_reward, self.last_generation)\n",
    "        \n",
    "        return True\n",
    "\n",
    "def rollout(env, policy):\n",
    "  \"\"\" play one agent vs the other in modified gym-style loop. \"\"\"\n",
    "  obs = env.reset()\n",
    " \n",
    "  done = False\n",
    "  total_reward = 0\n",
    "\n",
    "  while not done:\n",
    "\n",
    "    action, _states = policy.predict(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "\n",
    "    if RENDER_MODE:\n",
    "      env.render()\n",
    "\n",
    "  return total_reward\n",
    "\n",
    "def train():\n",
    "\n",
    "  vec_env = make_vec_env(SlimeVolleySelfPlayEnv, n_envs=n_cpu, seed=SEED)\n",
    "\n",
    "  model = A2C(\"MlpPolicy\", \n",
    "              vec_env, \n",
    "              learning_rate=learning_rate, \n",
    "              n_steps=n_steps, \n",
    "              gamma=gamma, \n",
    "              gae_lambda=gae_lambda, \n",
    "              ent_coef=ent_coef, \n",
    "              vf_coef=vf_coef, \n",
    "              max_grad_norm=max_grad_norm,\n",
    "              rms_prop_eps=rms_prop_eps, \n",
    "              use_rms_prop=use_rms_prop, \n",
    "              use_sde=use_sde, \n",
    "              sde_sample_freq=sde_sample_freq, \n",
    "              rollout_buffer_class=rollout_buffer_class, \n",
    "              rollout_buffer_kwargs=rollout_buffer_kwargs,\n",
    "              normalize_advantage=normalize_advantage, \n",
    "              stats_window_size=stats_window_size, \n",
    "              tensorboard_log=LOGDIR, \n",
    "              policy_kwargs=policy_kwargs, \n",
    "              verbose=verbose, \n",
    "              seed=SEED, \n",
    "              device=device,\n",
    "              _init_setup_model=_init_setup_model)\n",
    "\n",
    "  selfplay_eval_callback = SelfPlayCallback(vec_env,\n",
    "                  best_model_save_path=LOGDIR,\n",
    "                  log_path=LOGDIR,\n",
    "                  eval_freq=EVAL_FREQ,\n",
    "                  n_eval_episodes=EVAL_EPISODES_SELFPLAY)\n",
    "  \n",
    "  baseline_eval_callback = BaselineEvalCallback(model, \n",
    "                                                EVAL_FREQ, \n",
    "                                                EVAL_EPISODES_BASELINE, \n",
    "                                                n_cpu, \n",
    "                                                selfplay_eval_callback)\n",
    "\n",
    "  model.learn(total_timesteps=NUM_TIMESTEPS, callback=[selfplay_eval_callback, baseline_eval_callback])\n",
    "  model.save(os.path.join(LOGDIR, \"final_model\"))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "  train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
