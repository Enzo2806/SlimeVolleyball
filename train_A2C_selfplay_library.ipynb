{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insipired from: https://github.com/hardmaru/slimevolleygym/blob/master/training_scripts/train_ppo_selfplay.py\n",
    "\n",
    "import os\n",
    "import slimevolleygym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from shutil import copyfile # keep track of generations\n",
    "import torch\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from slimevolleygym import BaselinePolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "NUM_TIMESTEPS = int(5e7)\n",
    "EVAL_EPISODES_SELFPLAY = 100\n",
    "EVAL_EPISODES_BASELINE = 50\n",
    "EVAL_EPISODES_RANDOM = 10\n",
    "BEST_THRESHOLD = 0.5 # must achieve a mean score above this to replace prev best self\n",
    "RENDER_MODE = False # set this to false if you plan on running for full 1000 trials.\n",
    "n_cpu = 25\n",
    "EVAL_FREQ = 250000 // n_cpu\n",
    "learning_rate=0.0007\n",
    "n_steps=5\n",
    "gamma=0.99\n",
    "gae_lambda=1.0\n",
    "ent_coef=0.1\n",
    "vf_coef=0.5\n",
    "max_grad_norm=0.5\n",
    "rms_prop_eps=1e-05\n",
    "use_rms_prop=True\n",
    "use_sde=False\n",
    "sde_sample_freq=-1\n",
    "rollout_buffer_class=None\n",
    "rollout_buffer_kwargs=None\n",
    "normalize_advantage=False\n",
    "stats_window_size=100\n",
    "policy_kwargs=None\n",
    "verbose=1\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "_init_setup_model=True\n",
    "\n",
    "# Log dir\n",
    "LOGDIR = f\"./Logging/A2C-SELFPLAY-LIBRARY/{datetime.now().strftime('%Y%m%d-%H%M%S')}-lr-{learning_rate}-entcoef-{ent_coef}\"\n",
    "os.mkdir(LOGDIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Logging to ./Logging/A2C-SELFPLAY-LIBRARY/20240415-225338-lr-0.0007-entcoef-0.1/A2C_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 456      |\n",
      "|    ep_rew_mean        | -1.71    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5859     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.0728  |\n",
      "|    value_loss         | 0.0797   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 619      |\n",
      "|    ep_rew_mean        | -0.255   |\n",
      "| time/                 |          |\n",
      "|    fps                | 5908     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.86     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.022   |\n",
      "|    value_loss         | 0.0601   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 627      |\n",
      "|    ep_rew_mean        | -0.0707  |\n",
      "| time/                 |          |\n",
      "|    fps                | 5927     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 75000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.963    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | -0.0433  |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 638      |\n",
      "|    ep_rew_mean        | -0.07    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5935     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 16       |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.0141   |\n",
      "|    value_loss         | 0.0385   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 637      |\n",
      "|    ep_rew_mean        | 0.11     |\n",
      "| time/                 |          |\n",
      "|    fps                | 5922     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 125000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.0695  |\n",
      "|    value_loss         | 0.037    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 625      |\n",
      "|    ep_rew_mean        | 0.04     |\n",
      "| time/                 |          |\n",
      "|    fps                | 5898     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 150000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.981    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.0133  |\n",
      "|    value_loss         | 0.00768  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 629      |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5906     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 175000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.919    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.0363   |\n",
      "|    value_loss         | 0.0299   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 625      |\n",
      "|    ep_rew_mean        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5900     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | -0.03    |\n",
      "|    value_loss         | 0.0206   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 630      |\n",
      "|    ep_rew_mean        | 0.24     |\n",
      "| time/                 |          |\n",
      "|    fps                | 5878     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 225000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | -0.0235  |\n",
      "|    value_loss         | 0.0175   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=0.01 +/- 2.65\n",
      "Episode length: 632.27 +/- 111.82\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 632      |\n",
      "|    mean_reward        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 250000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0145   |\n",
      "|    value_loss         | 0.0107   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 625      |\n",
      "|    ep_rew_mean     | 0.2      |\n",
      "| time/              |          |\n",
      "|    fps             | 4668     |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 53       |\n",
      "|    total_timesteps | 250000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 623      |\n",
      "|    ep_rew_mean        | -0.22    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4760     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 57       |\n",
      "|    total_timesteps    | 275000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | 0.00243  |\n",
      "|    value_loss         | 0.0135   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 618      |\n",
      "|    ep_rew_mean        | -0.1     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4836     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 62       |\n",
      "|    total_timesteps    | 300000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | 0.0188   |\n",
      "|    value_loss         | 0.0168   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 621      |\n",
      "|    ep_rew_mean        | 0.1      |\n",
      "| time/                 |          |\n",
      "|    fps                | 4906     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 66       |\n",
      "|    total_timesteps    | 325000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.0195  |\n",
      "|    value_loss         | 0.0235   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 625      |\n",
      "|    ep_rew_mean        | 0.37     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4966     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 350000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.901    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.00588  |\n",
      "|    value_loss         | 0.0462   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 602      |\n",
      "|    ep_rew_mean        | 0.26     |\n",
      "| time/                 |          |\n",
      "|    fps                | 5015     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 74       |\n",
      "|    total_timesteps    | 375000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.0688   |\n",
      "|    value_loss         | 0.035    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 622      |\n",
      "|    ep_rew_mean        | -0.25    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5056     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 79       |\n",
      "|    total_timesteps    | 400000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.967    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | -0.0126  |\n",
      "|    value_loss         | 0.0117   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 632      |\n",
      "|    ep_rew_mean        | -0.12    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5100     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 83       |\n",
      "|    total_timesteps    | 425000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.0469  |\n",
      "|    value_loss         | 0.0496   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 638      |\n",
      "|    ep_rew_mean        | -0.04    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5141     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 87       |\n",
      "|    total_timesteps    | 450000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.997    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.0022   |\n",
      "|    value_loss         | 0.00118  |\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 107\u001b[0m\n\u001b[1;32m    104\u001b[0m   model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(LOGDIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 107\u001b[0m   \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 103\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m model \u001b[38;5;241m=\u001b[39m A2C(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     74\u001b[0m             vec_env, \n\u001b[1;32m     75\u001b[0m             learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     95\u001b[0m             _init_setup_model\u001b[38;5;241m=\u001b[39m_init_setup_model)\n\u001b[1;32m     97\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m SelfPlayCallback(vec_env,\n\u001b[1;32m     98\u001b[0m                 best_model_save_path\u001b[38;5;241m=\u001b[39mLOGDIR,\n\u001b[1;32m     99\u001b[0m                 log_path\u001b[38;5;241m=\u001b[39mLOGDIR,\n\u001b[1;32m    100\u001b[0m                 eval_freq\u001b[38;5;241m=\u001b[39mEVAL_FREQ,\n\u001b[1;32m    101\u001b[0m                 n_eval_episodes\u001b[38;5;241m=\u001b[39mEVAL_EPISODES)\n\u001b[0;32m--> 103\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(LOGDIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/a2c/a2c.py:201\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[1;32m    194\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfA2C:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:204\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    201\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m callback\u001b[38;5;241m.\u001b[39mon_step():\n\u001b[1;32m    202\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 204\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_info_buffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minfos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdones\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m n_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space, spaces\u001b[38;5;241m.\u001b[39mDiscrete):\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;66;03m# Reshape in case of discrete action\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:452\u001b[0m, in \u001b[0;36mBaseAlgorithm._update_info_buffer\u001b[0;34m(self, infos, dones)\u001b[0m\n\u001b[1;32m    450\u001b[0m     dones \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([\u001b[38;5;28;01mFalse\u001b[39;00m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(infos))\n\u001b[1;32m    451\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(infos):\n\u001b[0;32m--> 452\u001b[0m     maybe_ep_info \u001b[38;5;241m=\u001b[39m \u001b[43minfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mepisode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    453\u001b[0m     maybe_is_success \u001b[38;5;241m=\u001b[39m info\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_success\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    454\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m maybe_ep_info \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# wrapper over the normal single player env, but loads the best self play model\n",
    "class SlimeVolleySelfPlayEnv(slimevolleygym.SlimeVolleyEnv):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super(SlimeVolleySelfPlayEnv, self).__init__()\n",
    "    self.policy = self\n",
    "    self.best_model = None\n",
    "    self.best_model_filename = None\n",
    "\n",
    "  def predict(self, obs): # the policy\n",
    "    if self.best_model is None:\n",
    "      return self.action_space.sample() # return a random action\n",
    "    else:\n",
    "      action, _ = self.best_model.predict(obs)\n",
    "      return action\n",
    "\n",
    "  # load model if it's there\n",
    "  def reset(self):\n",
    "    modellist = [f for f in os.listdir(LOGDIR) if f.startswith(\"history\")]\n",
    "    modellist.sort()\n",
    "    if len(modellist) > 0:\n",
    "      filename = os.path.join(LOGDIR, modellist[-1]) # the latest best model\n",
    "      if filename != self.best_model_filename:\n",
    "        print(\"loading model: \", filename)\n",
    "        self.best_model_filename = filename\n",
    "        if self.best_model is not None:\n",
    "          del self.best_model\n",
    "        self.best_model = A2C.load(filename, env=self)\n",
    "    return super(SlimeVolleySelfPlayEnv, self).reset()\n",
    "\n",
    "# hacked it to only save new version of best model if beats prev self by BEST_THRESHOLD score\n",
    "# after saving model, resets the best score to be BEST_THRESHOLD\n",
    "class SelfPlayCallback(EvalCallback):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(SelfPlayCallback, self).__init__(*args, **kwargs)\n",
    "    self.best_mean_reward = BEST_THRESHOLD\n",
    "    self.generation = 0\n",
    "  def _on_step(self) -> bool:\n",
    "    result = super(SelfPlayCallback, self)._on_step()\n",
    "    if result and self.best_mean_reward > BEST_THRESHOLD:\n",
    "      self.generation += 1\n",
    "      print(\"SELFPLAY: mean_reward achieved:\", self.best_mean_reward)\n",
    "      print(\"SELFPLAY: new best model, bumping up generation to\", self.generation)\n",
    "      source_file = os.path.join(LOGDIR, \"best_model.zip\")\n",
    "      backup_file = os.path.join(LOGDIR, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n",
    "      copyfile(source_file, backup_file)\n",
    "      self.best_mean_reward = BEST_THRESHOLD\n",
    "    return result\n",
    "\n",
    "class BaselineEvalCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    Used to evaluate the agent against the baseline every certain number of iterations\n",
    "\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, vecenv, model, eval_freq, num_evals, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        self.model = model\n",
    "        self.eval_freq = eval_freq\n",
    "        self.num_evals = num_evals\n",
    "        self.baseline = BaselinePolicy()\n",
    "        self.vecenv = vecenv\n",
    "\n",
    "    def _on_training_start(self) -> None:\n",
    "        \"\"\"\n",
    "        This method is called before the first rollout starts.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_rollout_start(self) -> None:\n",
    "        \"\"\"\n",
    "        A rollout is the collection of environment interaction\n",
    "        using the current policy.\n",
    "        This event is triggered before collecting new samples.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "\n",
    "        if \n",
    "        \n",
    "        return True\n",
    "\n",
    "    def _on_rollout_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before updating the policy.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def _on_training_end(self) -> None:\n",
    "        \"\"\"\n",
    "        This event is triggered before exiting the `learn()` method.\n",
    "        \"\"\"\n",
    "        pass\n",
    "        \n",
    "def rollout(env, policy):\n",
    "  \"\"\" play one agent vs the other in modified gym-style loop. \"\"\"\n",
    "  obs = env.reset()\n",
    " \n",
    "  done = False\n",
    "  total_reward = 0\n",
    "\n",
    "  while not done:\n",
    "\n",
    "    action, _states = policy.predict(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "\n",
    "    if RENDER_MODE:\n",
    "      env.render()\n",
    "\n",
    "  return total_reward\n",
    "\n",
    "def train():\n",
    "\n",
    "  vec_env = make_vec_env(SlimeVolleySelfPlayEnv, n_envs=n_cpu, seed=SEED)\n",
    "\n",
    "  model = A2C(\"MlpPolicy\", \n",
    "              vec_env, \n",
    "              learning_rate=learning_rate, \n",
    "              n_steps=n_steps, \n",
    "              gamma=gamma, \n",
    "              gae_lambda=gae_lambda, \n",
    "              ent_coef=ent_coef, \n",
    "              vf_coef=vf_coef, \n",
    "              max_grad_norm=max_grad_norm,\n",
    "              rms_prop_eps=rms_prop_eps, \n",
    "              use_rms_prop=use_rms_prop, \n",
    "              use_sde=use_sde, \n",
    "              sde_sample_freq=sde_sample_freq, \n",
    "              rollout_buffer_class=rollout_buffer_class, \n",
    "              rollout_buffer_kwargs=rollout_buffer_kwargs,\n",
    "              normalize_advantage=normalize_advantage, \n",
    "              stats_window_size=stats_window_size, \n",
    "              tensorboard_log=LOGDIR, \n",
    "              policy_kwargs=policy_kwargs, \n",
    "              verbose=verbose, \n",
    "              seed=SEED, \n",
    "              device=device,\n",
    "              _init_setup_model=_init_setup_model)\n",
    "\n",
    "  selfplay_eval_callback = SelfPlayCallback(vec_env,\n",
    "                  best_model_save_path=LOGDIR,\n",
    "                  log_path=LOGDIR,\n",
    "                  eval_freq=EVAL_FREQ,\n",
    "                  n_eval_episodes=EVAL_EPISODES_SELFPLAY)\n",
    "\n",
    "  model.learn(total_timesteps=NUM_TIMESTEPS, callback=selfplay_eval_callback)\n",
    "  model.save(os.path.join(LOGDIR, \"final_model\"))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "  train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
