{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insipired from: https://github.com/hardmaru/slimevolleygym/blob/master/training_scripts/train_ppo_selfplay.py\n",
    "\n",
    "import os\n",
    "import slimevolleygym\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "from shutil import copyfile # keep track of generations\n",
    "import torch\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from slimevolleygym import BaselinePolicy\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 17\n",
    "NUM_TIMESTEPS = int(5e7)\n",
    "EVAL_EPISODES_SELFPLAY = 100\n",
    "EVAL_EPISODES_BASELINE = 50\n",
    "EVAL_EPISODES_RANDOM = 10\n",
    "BEST_THRESHOLD = 0.5 # must achieve a mean score above this to replace prev best self\n",
    "RENDER_MODE = False # set this to false if you plan on running for full 1000 trials.\n",
    "n_cpu = 50\n",
    "EVAL_FREQ = 250000 // n_cpu\n",
    "learning_rate=0.0007\n",
    "n_steps=5\n",
    "gamma=0.99\n",
    "gae_lambda=1.0\n",
    "ent_coef=0.1\n",
    "vf_coef=0.5\n",
    "max_grad_norm=0.5\n",
    "rms_prop_eps=1e-05\n",
    "use_rms_prop=True\n",
    "use_sde=False\n",
    "sde_sample_freq=-1\n",
    "rollout_buffer_class=None\n",
    "rollout_buffer_kwargs=None\n",
    "normalize_advantage=False\n",
    "stats_window_size=100\n",
    "policy_kwargs=None\n",
    "verbose=1\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "_init_setup_model=True\n",
    "\n",
    "# Log dir\n",
    "LOGDIR = f\"./Logging/A2C-SELFPLAY-LIBRARY/{datetime.now().strftime('%Y%m%d-%H%M%S')}-lr-{learning_rate}-entcoef-{ent_coef}\"\n",
    "os.mkdir(LOGDIR) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Logging to ./Logging/A2C-SELFPLAY-LIBRARY/20240416-120029-lr-0.0007-entcoef-0.1/A2C_1\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 447      |\n",
      "|    ep_rew_mean        | -2.83    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5508     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 4        |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.878    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | -0.0749  |\n",
      "|    value_loss         | 0.0563   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 624      |\n",
      "|    ep_rew_mean        | -0.0588  |\n",
      "| time/                 |          |\n",
      "|    fps                | 5715     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 8        |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.792    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | 0.022    |\n",
      "|    value_loss         | 0.0565   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 635      |\n",
      "|    ep_rew_mean        | 0.2      |\n",
      "| time/                 |          |\n",
      "|    fps                | 5784     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 12       |\n",
      "|    total_timesteps    | 75000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.0433   |\n",
      "|    value_loss         | 0.0129   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 648      |\n",
      "|    ep_rew_mean        | 0.02     |\n",
      "| time/                 |          |\n",
      "|    fps                | 5820     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 17       |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.825    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.0863   |\n",
      "|    value_loss         | 0.088    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 631      |\n",
      "|    ep_rew_mean        | -0.21    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5842     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 21       |\n",
      "|    total_timesteps    | 125000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | 0.0682   |\n",
      "|    value_loss         | 0.0373   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 625      |\n",
      "|    ep_rew_mean        | 0.24     |\n",
      "| time/                 |          |\n",
      "|    fps                | 5857     |\n",
      "|    iterations         | 600      |\n",
      "|    time_elapsed       | 25       |\n",
      "|    total_timesteps    | 150000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.965    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 599      |\n",
      "|    policy_loss        | -0.0178  |\n",
      "|    value_loss         | 0.0162   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 624      |\n",
      "|    ep_rew_mean        | -0.01    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5867     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 175000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.975    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00801  |\n",
      "|    value_loss         | 0.00695  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 625      |\n",
      "|    ep_rew_mean        | 0.07     |\n",
      "| time/                 |          |\n",
      "|    fps                | 5874     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 34       |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.865    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.0365   |\n",
      "|    value_loss         | 0.0539   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 635      |\n",
      "|    ep_rew_mean        | -0.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5880     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 38       |\n",
      "|    total_timesteps    | 225000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.0289   |\n",
      "|    value_loss         | 0.0167   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=250000, episode_reward=0.09 +/- 2.54\n",
      "Episode length: 660.66 +/- 115.55\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 661      |\n",
      "|    mean_reward        | 0.09     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 250000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0822   |\n",
      "|    value_loss         | 0.0449   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 622      |\n",
      "|    ep_rew_mean     | -0.07    |\n",
      "| time/              |          |\n",
      "|    fps             | 4230     |\n",
      "|    iterations      | 1000     |\n",
      "|    time_elapsed    | 59       |\n",
      "|    total_timesteps | 250000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 626      |\n",
      "|    ep_rew_mean        | -0.39    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4343     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 275000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.918    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0104  |\n",
      "|    value_loss         | 0.0306   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 644      |\n",
      "|    ep_rew_mean        | -0.44    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4442     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 67       |\n",
      "|    total_timesteps    | 300000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.881    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.0266  |\n",
      "|    value_loss         | 0.0651   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 641      |\n",
      "|    ep_rew_mean        | 0        |\n",
      "| time/                 |          |\n",
      "|    fps                | 4529     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 71       |\n",
      "|    total_timesteps    | 325000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.884    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | -0.0375  |\n",
      "|    value_loss         | 0.0594   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 623      |\n",
      "|    ep_rew_mean        | 0.33     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4606     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 75       |\n",
      "|    total_timesteps    | 350000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | -0.0601  |\n",
      "|    value_loss         | 0.0303   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 605      |\n",
      "|    ep_rew_mean        | 0.44     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4676     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 80       |\n",
      "|    total_timesteps    | 375000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | 0.0247   |\n",
      "|    value_loss         | 0.029    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 608      |\n",
      "|    ep_rew_mean        | -0.13    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4739     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 84       |\n",
      "|    total_timesteps    | 400000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.0255   |\n",
      "|    value_loss         | 0.0151   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 618      |\n",
      "|    ep_rew_mean        | -0.21    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4795     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 425000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | -0.0616  |\n",
      "|    value_loss         | 0.0288   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 631      |\n",
      "|    ep_rew_mean        | -0.33    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4847     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 92       |\n",
      "|    total_timesteps    | 450000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.951    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | -0.0251  |\n",
      "|    value_loss         | 0.0241   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 623      |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4894     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 97       |\n",
      "|    total_timesteps    | 475000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.889    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | -0.0222  |\n",
      "|    value_loss         | 0.0279   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-0.05 +/- 2.71\n",
      "Episode length: 633.66 +/- 107.73\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 634      |\n",
      "|    mean_reward        | -0.05    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 500000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.0114  |\n",
      "|    value_loss         | 0.0362   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 618      |\n",
      "|    ep_rew_mean     | -0.12    |\n",
      "| time/              |          |\n",
      "|    fps             | 4267     |\n",
      "|    iterations      | 2000     |\n",
      "|    time_elapsed    | 117      |\n",
      "|    total_timesteps | 500000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 621      |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4325     |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 121      |\n",
      "|    total_timesteps    | 525000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 0.0324   |\n",
      "|    value_loss         | 0.0271   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 608      |\n",
      "|    ep_rew_mean        | -0.15    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4379     |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 125      |\n",
      "|    total_timesteps    | 550000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.907    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | 0.0228   |\n",
      "|    value_loss         | 0.0528   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 624      |\n",
      "|    ep_rew_mean        | -0.29    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4429     |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 129      |\n",
      "|    total_timesteps    | 575000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.952    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | 0.0541   |\n",
      "|    value_loss         | 0.0198   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 630      |\n",
      "|    ep_rew_mean        | -0.39    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4476     |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 134      |\n",
      "|    total_timesteps    | 600000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | -0.0397  |\n",
      "|    value_loss         | 0.0233   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 638      |\n",
      "|    ep_rew_mean        | 0.09     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4520     |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 138      |\n",
      "|    total_timesteps    | 625000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.903    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 0.0577   |\n",
      "|    value_loss         | 0.0438   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 636      |\n",
      "|    ep_rew_mean        | -0.21    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4562     |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 142      |\n",
      "|    total_timesteps    | 650000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.959    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -0.0363  |\n",
      "|    value_loss         | 0.0166   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 635      |\n",
      "|    ep_rew_mean        | -0.06    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4601     |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 146      |\n",
      "|    total_timesteps    | 675000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.982    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.0179  |\n",
      "|    value_loss         | 0.00681  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 642      |\n",
      "|    ep_rew_mean        | 0.01     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4638     |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 150      |\n",
      "|    total_timesteps    | 700000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.916    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | -0.0432  |\n",
      "|    value_loss         | 0.0387   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 634      |\n",
      "|    ep_rew_mean        | 0.07     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4673     |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 155      |\n",
      "|    total_timesteps    | 725000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.942    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | -0.037   |\n",
      "|    value_loss         | 0.023    |\n",
      "------------------------------------\n",
      "Eval num_timesteps=750000, episode_reward=0.24 +/- 2.65\n",
      "Episode length: 644.82 +/- 115.58\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 645      |\n",
      "|    mean_reward        | 0.24     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 750000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | 0.00325  |\n",
      "|    value_loss         | 0.0207   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 644      |\n",
      "|    ep_rew_mean     | -0.03    |\n",
      "| time/              |          |\n",
      "|    fps             | 4257     |\n",
      "|    iterations      | 3000     |\n",
      "|    time_elapsed    | 176      |\n",
      "|    total_timesteps | 750000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 632      |\n",
      "|    ep_rew_mean        | 0.22     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4296     |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 180      |\n",
      "|    total_timesteps    | 775000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | 0.0391   |\n",
      "|    value_loss         | 0.0217   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 622      |\n",
      "|    ep_rew_mean        | 0.03     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4333     |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 184      |\n",
      "|    total_timesteps    | 800000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | -0.0983  |\n",
      "|    value_loss         | 0.0481   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 628      |\n",
      "|    ep_rew_mean        | 0.1      |\n",
      "| time/                 |          |\n",
      "|    fps                | 4369     |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 188      |\n",
      "|    total_timesteps    | 825000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 0.0378   |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 626      |\n",
      "|    ep_rew_mean        | 0.41     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4403     |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 193      |\n",
      "|    total_timesteps    | 850000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.943    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -0.0142  |\n",
      "|    value_loss         | 0.0188   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 628      |\n",
      "|    ep_rew_mean        | 0.28     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4436     |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 197      |\n",
      "|    total_timesteps    | 875000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.94     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | 0.0132   |\n",
      "|    value_loss         | 0.029    |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 622       |\n",
      "|    ep_rew_mean        | -0.11     |\n",
      "| time/                 |           |\n",
      "|    fps                | 4467      |\n",
      "|    iterations         | 3600      |\n",
      "|    time_elapsed       | 201       |\n",
      "|    total_timesteps    | 900000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0.944     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 3599      |\n",
      "|    policy_loss        | -0.000289 |\n",
      "|    value_loss         | 0.0241    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 627      |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4497     |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 205      |\n",
      "|    total_timesteps    | 925000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.964    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | 0.00553  |\n",
      "|    value_loss         | 0.00999  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 640      |\n",
      "|    ep_rew_mean        | -0.2     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4525     |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 209      |\n",
      "|    total_timesteps    | 950000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.0344   |\n",
      "|    value_loss         | 0.0424   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 624      |\n",
      "|    ep_rew_mean        | -0.09    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4553     |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 214      |\n",
      "|    total_timesteps    | 975000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.913    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | 0.0258   |\n",
      "|    value_loss         | 0.0366   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-0.48 +/- 2.74\n",
      "Episode length: 622.80 +/- 103.08\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 623      |\n",
      "|    mean_reward        | -0.48    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.902    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | 0.0291   |\n",
      "|    value_loss         | 0.0331   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 607      |\n",
      "|    ep_rew_mean     | 0.37     |\n",
      "| time/              |          |\n",
      "|    fps             | 4257     |\n",
      "|    iterations      | 4000     |\n",
      "|    time_elapsed    | 234      |\n",
      "|    total_timesteps | 1000000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 615      |\n",
      "|    ep_rew_mean        | 0.44     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4287     |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 239      |\n",
      "|    total_timesteps    | 1025000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.954    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | 0.0034   |\n",
      "|    value_loss         | 0.0182   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 613      |\n",
      "|    ep_rew_mean        | 0.28     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4315     |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 243      |\n",
      "|    total_timesteps    | 1050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.894    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | 7.96e-05 |\n",
      "|    value_loss         | 0.0453   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 634      |\n",
      "|    ep_rew_mean        | 0.09     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4343     |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 247      |\n",
      "|    total_timesteps    | 1075000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.989    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -0.00175 |\n",
      "|    value_loss         | 0.00318  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 623      |\n",
      "|    ep_rew_mean        | 0.04     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4369     |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 251      |\n",
      "|    total_timesteps    | 1100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.864    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 0.0469   |\n",
      "|    value_loss         | 0.0495   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 620      |\n",
      "|    ep_rew_mean        | -0.24    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4395     |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 255      |\n",
      "|    total_timesteps    | 1125000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.99     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | -0.00693 |\n",
      "|    value_loss         | 0.00244  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 625      |\n",
      "|    ep_rew_mean        | -0.05    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4419     |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 260      |\n",
      "|    total_timesteps    | 1150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.871    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -0.0749  |\n",
      "|    value_loss         | 0.0684   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 633      |\n",
      "|    ep_rew_mean        | 0.1      |\n",
      "| time/                 |          |\n",
      "|    fps                | 4443     |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 264      |\n",
      "|    total_timesteps    | 1175000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.935    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | -0.0315  |\n",
      "|    value_loss         | 0.0345   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 637      |\n",
      "|    ep_rew_mean        | 0.32     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4467     |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 268      |\n",
      "|    total_timesteps    | 1200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.054    |\n",
      "|    value_loss         | 0.0404   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 645      |\n",
      "|    ep_rew_mean        | -0.21    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4489     |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 272      |\n",
      "|    total_timesteps    | 1225000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | -0.044   |\n",
      "|    value_loss         | 0.0321   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1250000, episode_reward=0.00 +/- 2.60\n",
      "Episode length: 643.47 +/- 99.41\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 643      |\n",
      "|    mean_reward        | 0        |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.911    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -0.0447  |\n",
      "|    value_loss         | 0.0376   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 648      |\n",
      "|    ep_rew_mean     | -0.46    |\n",
      "| time/              |          |\n",
      "|    fps             | 4241     |\n",
      "|    iterations      | 5000     |\n",
      "|    time_elapsed    | 294      |\n",
      "|    total_timesteps | 1250000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 634      |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4264     |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 298      |\n",
      "|    total_timesteps    | 1275000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.931    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | 0.0267   |\n",
      "|    value_loss         | 0.0232   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 617      |\n",
      "|    ep_rew_mean        | 0        |\n",
      "| time/                 |          |\n",
      "|    fps                | 4288     |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 303      |\n",
      "|    total_timesteps    | 1300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.914    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | 0.0204   |\n",
      "|    value_loss         | 0.0313   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 622      |\n",
      "|    ep_rew_mean        | 0.11     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4310     |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 307      |\n",
      "|    total_timesteps    | 1325000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.896    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | 0.0283   |\n",
      "|    value_loss         | 0.0597   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 623      |\n",
      "|    ep_rew_mean        | 0.04     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4332     |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 311      |\n",
      "|    total_timesteps    | 1350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.945    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | -0.00745 |\n",
      "|    value_loss         | 0.0288   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 619      |\n",
      "|    ep_rew_mean        | 0.34     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4353     |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 315      |\n",
      "|    total_timesteps    | 1375000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.968    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 0.00256  |\n",
      "|    value_loss         | 0.0133   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 635      |\n",
      "|    ep_rew_mean        | 0.51     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4373     |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 320      |\n",
      "|    total_timesteps    | 1400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | -0.0285  |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 636      |\n",
      "|    ep_rew_mean        | 0.54     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4394     |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 324      |\n",
      "|    total_timesteps    | 1425000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.934    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -0.048   |\n",
      "|    value_loss         | 0.0316   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 625      |\n",
      "|    ep_rew_mean        | 0.17     |\n",
      "| time/                 |          |\n",
      "|    fps                | 4413     |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 328      |\n",
      "|    total_timesteps    | 1450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | -0.025   |\n",
      "|    value_loss         | 0.0326   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 629      |\n",
      "|    ep_rew_mean        | -0.11    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4432     |\n",
      "|    iterations         | 5900     |\n",
      "|    time_elapsed       | 332      |\n",
      "|    total_timesteps    | 1475000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.969    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5899     |\n",
      "|    policy_loss        | -0.0273  |\n",
      "|    value_loss         | 0.00863  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1500000, episode_reward=-0.02 +/- 2.84\n",
      "Episode length: 636.31 +/- 127.36\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 636      |\n",
      "|    mean_reward        | -0.02    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1500000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.933    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5999     |\n",
      "|    policy_loss        | 0.0448   |\n",
      "|    value_loss         | 0.0307   |\n",
      "------------------------------------\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 633      |\n",
      "|    ep_rew_mean     | -0.39    |\n",
      "| time/              |          |\n",
      "|    fps             | 4236     |\n",
      "|    iterations      | 6000     |\n",
      "|    time_elapsed    | 354      |\n",
      "|    total_timesteps | 1500000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 619      |\n",
      "|    ep_rew_mean        | -0.26    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4256     |\n",
      "|    iterations         | 6100     |\n",
      "|    time_elapsed       | 358      |\n",
      "|    total_timesteps    | 1525000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6099     |\n",
      "|    policy_loss        | 0.0447   |\n",
      "|    value_loss         | 0.0173   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 617      |\n",
      "|    ep_rew_mean        | -0.38    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4276     |\n",
      "|    iterations         | 6200     |\n",
      "|    time_elapsed       | 362      |\n",
      "|    total_timesteps    | 1550000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.944    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6199     |\n",
      "|    policy_loss        | 0.00134  |\n",
      "|    value_loss         | 0.0289   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 636      |\n",
      "|    ep_rew_mean        | -0.75    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4294     |\n",
      "|    iterations         | 6300     |\n",
      "|    time_elapsed       | 366      |\n",
      "|    total_timesteps    | 1575000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.898    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6299     |\n",
      "|    policy_loss        | -0.0355  |\n",
      "|    value_loss         | 0.0388   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 647      |\n",
      "|    ep_rew_mean        | -0.55    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4313     |\n",
      "|    iterations         | 6400     |\n",
      "|    time_elapsed       | 370      |\n",
      "|    total_timesteps    | 1600000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6399     |\n",
      "|    policy_loss        | -0.0504  |\n",
      "|    value_loss         | 0.0215   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 654      |\n",
      "|    ep_rew_mean        | -0.23    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4331     |\n",
      "|    iterations         | 6500     |\n",
      "|    time_elapsed       | 375      |\n",
      "|    total_timesteps    | 1625000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.995    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6499     |\n",
      "|    policy_loss        | 0.0117   |\n",
      "|    value_loss         | 0.00165  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 654      |\n",
      "|    ep_rew_mean        | -0.24    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4348     |\n",
      "|    iterations         | 6600     |\n",
      "|    time_elapsed       | 379      |\n",
      "|    total_timesteps    | 1650000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.962    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6599     |\n",
      "|    policy_loss        | 0.0239   |\n",
      "|    value_loss         | 0.0112   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 650      |\n",
      "|    ep_rew_mean        | -0.38    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4366     |\n",
      "|    iterations         | 6700     |\n",
      "|    time_elapsed       | 383      |\n",
      "|    total_timesteps    | 1675000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.917    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6699     |\n",
      "|    policy_loss        | 0.0305   |\n",
      "|    value_loss         | 0.0328   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 641      |\n",
      "|    ep_rew_mean        | -0.33    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4383     |\n",
      "|    iterations         | 6800     |\n",
      "|    time_elapsed       | 387      |\n",
      "|    total_timesteps    | 1700000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.958    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6799     |\n",
      "|    policy_loss        | 0.0603   |\n",
      "|    value_loss         | 0.0215   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 624      |\n",
      "|    ep_rew_mean        | -0.17    |\n",
      "| time/                 |          |\n",
      "|    fps                | 4399     |\n",
      "|    iterations         | 6900     |\n",
      "|    time_elapsed       | 392      |\n",
      "|    total_timesteps    | 1725000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.929    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6899     |\n",
      "|    policy_loss        | -0.0125  |\n",
      "|    value_loss         | 0.0369   |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1750000, episode_reward=0.51 +/- 2.64\n",
      "Episode length: 627.91 +/- 100.86\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 628      |\n",
      "|    mean_reward        | 0.51     |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1750000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.956    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 6999     |\n",
      "|    policy_loss        | 0.0122   |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "SELFPLAY: mean_reward achieved: 0.51\n",
      "SELFPLAY: new best model, bumping up generation to 1\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 615      |\n",
      "|    ep_rew_mean     | -0.29    |\n",
      "| time/              |          |\n",
      "|    fps             | 4244     |\n",
      "|    iterations      | 7000     |\n",
      "|    time_elapsed    | 412      |\n",
      "|    total_timesteps | 1750000  |\n",
      "---------------------------------\n",
      "loading model:  ./Logging/A2C-SELFPLAY-LIBRARY/20240416-120029-lr-0.0007-entcoef-0.1/history_00000001.zip\n"
     ]
    },
    {
     "ename": "UnpicklingError",
     "evalue": "Weights only load failed. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution.Do it only if you get the file from a trusted source. WeightsUnpickler error: Unsupported operand 71",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 169\u001b[0m\n\u001b[1;32m    166\u001b[0m   model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(LOGDIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[3], line 165\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    153\u001b[0m selfplay_eval_callback \u001b[38;5;241m=\u001b[39m SelfPlayCallback(vec_env,\n\u001b[1;32m    154\u001b[0m                 best_model_save_path\u001b[38;5;241m=\u001b[39mLOGDIR,\n\u001b[1;32m    155\u001b[0m                 log_path\u001b[38;5;241m=\u001b[39mLOGDIR,\n\u001b[1;32m    156\u001b[0m                 eval_freq\u001b[38;5;241m=\u001b[39mEVAL_FREQ,\n\u001b[1;32m    157\u001b[0m                 n_eval_episodes\u001b[38;5;241m=\u001b[39mEVAL_EPISODES_SELFPLAY)\n\u001b[1;32m    159\u001b[0m baseline_eval_callback \u001b[38;5;241m=\u001b[39m BaselineEvalCallback(model, \n\u001b[1;32m    160\u001b[0m                                               EVAL_FREQ, \n\u001b[1;32m    161\u001b[0m                                               EVAL_EPISODES_BASELINE, \n\u001b[1;32m    162\u001b[0m                                               n_cpu, \n\u001b[1;32m    163\u001b[0m                                               selfplay_eval_callback)\n\u001b[0;32m--> 165\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mselfplay_eval_callback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline_eval_callback\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    166\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(LOGDIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/a2c/a2c.py:201\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[1;32m    194\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfA2C:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:70\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     67\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx]:\n\u001b[1;32m     68\u001b[0m         \u001b[38;5;66;03m# save final observation where user can get it, then reset\u001b[39;00m\n\u001b[1;32m     69\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mterminal_observation\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[0;32m---> 70\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreset_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_save_obs(env_idx, obs)\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_from_buf(), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews), np\u001b[38;5;241m.\u001b[39mcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones), deepcopy(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:83\u001b[0m, in \u001b[0;36mMonitor.reset\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected you to pass keyword argument \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m into reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcurrent_reset_info[key] \u001b[38;5;241m=\u001b[39m value\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/shimmy/openai_gym_compatibility.py:235\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.reset\u001b[0;34m(self, seed, options)\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m options \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    231\u001b[0m     warn(\n\u001b[1;32m    232\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGym v21 environment do not accept options as a reset parameter, options=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moptions\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    233\u001b[0m     )\n\u001b[0;32m--> 235\u001b[0m obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    238\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "Cell \u001b[0;32mIn[3], line 28\u001b[0m, in \u001b[0;36mSlimeVolleySelfPlayEnv.reset\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     27\u001b[0m       \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model\n\u001b[0;32m---> 28\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbest_model \u001b[38;5;241m=\u001b[39m \u001b[43mA2C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m(SlimeVolleySelfPlayEnv, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mreset()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/base_class.py:680\u001b[0m, in \u001b[0;36mBaseAlgorithm.load\u001b[0;34m(cls, path, env, device, custom_objects, print_system_info, force_reset, **kwargs)\u001b[0m\n\u001b[1;32m    677\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m== CURRENT SYSTEM INFO ==\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    678\u001b[0m     get_system_info()\n\u001b[0;32m--> 680\u001b[0m data, params, pytorch_variables \u001b[38;5;241m=\u001b[39m \u001b[43mload_from_zip_file\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    681\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    682\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    683\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcustom_objects\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_objects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    684\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprint_system_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_system_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    685\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    687\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo data found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    688\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo params found in the saved file\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/save_util.py:450\u001b[0m, in \u001b[0;36mload_from_zip_file\u001b[0;34m(load_path, load_data, custom_objects, device, verbose, print_system_info)\u001b[0m\n\u001b[1;32m    447\u001b[0m file_content\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    448\u001b[0m \u001b[38;5;66;03m# Load the parameters with the right ``map_location``.\u001b[39;00m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;66;03m# Remove \".pth\" ending with splitext\u001b[39;00m\n\u001b[0;32m--> 450\u001b[0m th_object \u001b[38;5;241m=\u001b[39m \u001b[43mth\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    451\u001b[0m \u001b[38;5;66;03m# \"tensors.pth\" was renamed \"pytorch_variables.pth\" in v0.9.0, see PR #138\u001b[39;00m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file_path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpytorch_variables.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m file_path \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtensors.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;66;03m# PyTorch variables (not state_dicts)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/torch/serialization.py:788\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    786\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, _weights_only_unpickler, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    787\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m--> 788\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m    790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution.Do it only if you get the file from a trusted source. WeightsUnpickler error: Unsupported operand 71"
     ]
    }
   ],
   "source": [
    "# wrapper over the normal single player env, but loads the best self play model\n",
    "class SlimeVolleySelfPlayEnv(slimevolleygym.SlimeVolleyEnv):\n",
    "  \n",
    "  def __init__(self):\n",
    "    super(SlimeVolleySelfPlayEnv, self).__init__()\n",
    "    self.policy = self\n",
    "    self.best_model = None\n",
    "    self.best_model_filename = None\n",
    "\n",
    "  def predict(self, obs): # the policy\n",
    "    if self.best_model is None:\n",
    "      return self.action_space.sample() # return a random action\n",
    "    else:\n",
    "      action, _ = self.best_model.predict(obs)\n",
    "      return action\n",
    "\n",
    "  # load model if it's there\n",
    "  def reset(self):\n",
    "    modellist = [f for f in os.listdir(LOGDIR) if f.startswith(\"history\")]\n",
    "    modellist.sort()\n",
    "    if len(modellist) > 0:\n",
    "      filename = os.path.join(LOGDIR, modellist[-1]) # the latest best model\n",
    "      if filename != self.best_model_filename:\n",
    "        self.best_model_filename = filename\n",
    "        if self.best_model is not None:\n",
    "          del self.best_model\n",
    "        self.best_model = A2C.load(filename, env=self, weights_only=False)\n",
    "    return super(SlimeVolleySelfPlayEnv, self).reset()\n",
    "\n",
    "# hacked it to only save new version of best model if beats prev self by BEST_THRESHOLD score\n",
    "# after saving model, resets the best score to be BEST_THRESHOLD\n",
    "class SelfPlayCallback(EvalCallback):\n",
    "  def __init__(self, *args, **kwargs):\n",
    "    super(SelfPlayCallback, self).__init__(*args, **kwargs)\n",
    "    self.best_mean_reward = BEST_THRESHOLD\n",
    "    self.generation = 0\n",
    "  def _on_step(self) -> bool:\n",
    "    result = super(SelfPlayCallback, self)._on_step()\n",
    "    if result and self.best_mean_reward > BEST_THRESHOLD:\n",
    "      self.generation += 1\n",
    "      source_file = os.path.join(LOGDIR, \"best_model.zip\")\n",
    "      backup_file = os.path.join(LOGDIR, \"history_\"+str(self.generation).zfill(8)+\".zip\")\n",
    "      copyfile(source_file, backup_file)\n",
    "      self.best_mean_reward = BEST_THRESHOLD\n",
    "    return result\n",
    "\n",
    "class BaselineEvalCallback(BaseCallback):\n",
    "    \"\"\"\n",
    "    A custom callback that derives from ``BaseCallback``.\n",
    "\n",
    "    Used to evaluate the agent against the baseline every certain number of iterations\n",
    "\n",
    "    :param verbose: Verbosity level: 0 for no output, 1 for info messages, 2 for debug messages\n",
    "    \"\"\"\n",
    "    def __init__(self, model, eval_freq, num_evals, n_cpu, selfplaycallback, verbose: int = 0):\n",
    "        super().__init__(verbose)\n",
    "        \n",
    "        # Store the instance variables\n",
    "        self.model = model\n",
    "        self.eval_freq = eval_freq\n",
    "        self.num_evals = num_evals\n",
    "        self.baseline = BaselinePolicy()\n",
    "        self.last_generation = 0\n",
    "        self.selfplaycallback = selfplaycallback # Store the selfplay callback to access the generation number\n",
    "\n",
    "        # Make the regular environment with the opponent being the baseline instead of selfplay for this callback\n",
    "        self.vecenv = make_vec_env(slimevolleygym.SlimeVolleyEnv, n_envs=n_cpu, seed=SEED)\n",
    "\n",
    "        # Create a summarywriter at the logdir\n",
    "        self.writer = SummaryWriter(log_dir=LOGDIR)\n",
    "        \n",
    "    def _on_step(self) -> bool:\n",
    "        \"\"\"\n",
    "        This method will be called by the model after each call to `env.step()`.\n",
    "\n",
    "        For child callback (of an `EventCallback`), this will be called\n",
    "        when the event is triggered.\n",
    "\n",
    "        :return: If the callback returns False, training is aborted early.\n",
    "        \"\"\"\n",
    "\n",
    "        if self.n_calls > 0 and self.n_calls % self.eval_freq == 0:\n",
    "            \n",
    "            # Evaluate the model in the vectorized environment\n",
    "            mean_reward, std_reward = evaluate_policy(self.model, self.vecenv, n_eval_episodes=self.num_evals)\n",
    "\n",
    "            # Log the mean and std reward\n",
    "            self.writer.add_scalar(\"Average baseline test return - Training step\", mean_reward, self.n_calls)\n",
    "            self.writer.add_scalar(\"Baseline test return standard deviation - Training step\", std_reward, self.n_calls)\n",
    "\n",
    "            # If the generation number increased\n",
    "            if self.selfplaycallback.generation > self.last_generation:\n",
    "                \n",
    "                # Update the generation number to match\n",
    "                self.last_generation = self.selfplaycallback.generation\n",
    "               \n",
    "                # Log the mean and std reward\n",
    "                self.writer.add_scalar(\"Average baseline test return - Generation\", mean_reward, self.last_generation)\n",
    "                self.writer.add_scalar(\"Baseline test return standard deviation - Generation\", mean_reward, self.last_generation)\n",
    "        \n",
    "        return True\n",
    "\n",
    "def rollout(env, policy):\n",
    "  \"\"\" play one agent vs the other in modified gym-style loop. \"\"\"\n",
    "  obs = env.reset()\n",
    " \n",
    "  done = False\n",
    "  total_reward = 0\n",
    "\n",
    "  while not done:\n",
    "\n",
    "    action, _states = policy.predict(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "\n",
    "    total_reward += reward\n",
    "\n",
    "    if RENDER_MODE:\n",
    "      env.render()\n",
    "\n",
    "  return total_reward\n",
    "\n",
    "def train():\n",
    "\n",
    "  vec_env = make_vec_env(SlimeVolleySelfPlayEnv, n_envs=n_cpu, seed=SEED)\n",
    "\n",
    "  model = A2C(\"MlpPolicy\", \n",
    "              vec_env, \n",
    "              learning_rate=learning_rate, \n",
    "              n_steps=n_steps, \n",
    "              gamma=gamma, \n",
    "              gae_lambda=gae_lambda, \n",
    "              ent_coef=ent_coef, \n",
    "              vf_coef=vf_coef, \n",
    "              max_grad_norm=max_grad_norm,\n",
    "              rms_prop_eps=rms_prop_eps, \n",
    "              use_rms_prop=use_rms_prop, \n",
    "              use_sde=use_sde, \n",
    "              sde_sample_freq=sde_sample_freq, \n",
    "              rollout_buffer_class=rollout_buffer_class, \n",
    "              rollout_buffer_kwargs=rollout_buffer_kwargs,\n",
    "              normalize_advantage=normalize_advantage, \n",
    "              stats_window_size=stats_window_size, \n",
    "              tensorboard_log=LOGDIR, \n",
    "              policy_kwargs=policy_kwargs, \n",
    "              verbose=verbose, \n",
    "              seed=SEED, \n",
    "              device=device,\n",
    "              _init_setup_model=_init_setup_model)\n",
    "\n",
    "  selfplay_eval_callback = SelfPlayCallback(vec_env,\n",
    "                  best_model_save_path=LOGDIR,\n",
    "                  log_path=LOGDIR,\n",
    "                  eval_freq=EVAL_FREQ,\n",
    "                  n_eval_episodes=EVAL_EPISODES_SELFPLAY)\n",
    "  \n",
    "  baseline_eval_callback = BaselineEvalCallback(model, \n",
    "                                                EVAL_FREQ, \n",
    "                                                EVAL_EPISODES_BASELINE, \n",
    "                                                n_cpu, \n",
    "                                                selfplay_eval_callback)\n",
    "\n",
    "  model.learn(total_timesteps=NUM_TIMESTEPS, callback=[selfplay_eval_callback, baseline_eval_callback])\n",
    "  model.save(os.path.join(LOGDIR, \"final_model\"))\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "  train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
