{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.PPO.PPO_Agent import PPO_Agent\n",
    "from Models.DDQN.DDQN_Agent import DDQN_Agent\n",
    "from Models.DDQN.PRB import PrioritizedReplayBuffer\n",
    "from stable_baselines3 import A2C\n",
    "import slimevolleygym.mlp as mlp\n",
    "from slimevolleygym.mlp import Model\n",
    "import torch\n",
    "import slimevolleygym\n",
    "from slimevolleygym import BaselinePolicy\n",
    "from utils import convert_to_vector, convert_to_value, convert_list_to_vectors\n",
    "import types\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Print the device as a check\n",
    "print(\"Device used: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the models as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.22631-SP0 10.0.22631\n",
      "- Python: 3.10.4\n",
      "- Stable-Baselines3: 2.3.0\n",
      "- PyTorch: 2.2.2+cu118\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.23.1\n",
      "- Cloudpickle: 3.0.0\n",
      "- Gymnasium: 0.29.1\n",
      "- OpenAI Gym: 0.21.0\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Linux-5.4.0-144-generic-x86_64-with-glibc2.27 # 161~18.04.1-Ubuntu SMP Fri Feb 10 15:55:22 UTC 2023\n",
      "- Python: 3.9.12\n",
      "- Stable-Baselines3: 2.3.0\n",
      "- PyTorch: 1.13.0+cu116\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 2.2.1\n",
      "- Gymnasium: 0.29.1\n",
      "- OpenAI Gym: 0.26.2\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.22631-SP0 10.0.22631\n",
      "- Python: 3.10.4\n",
      "- Stable-Baselines3: 2.3.0\n",
      "- PyTorch: 2.2.2+cu118\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.23.1\n",
      "- Cloudpickle: 3.0.0\n",
      "- Gymnasium: 0.29.1\n",
      "- OpenAI Gym: 0.21.0\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Linux-5.4.0-144-generic-x86_64-with-glibc2.27 # 161~18.04.1-Ubuntu SMP Fri Feb 10 15:55:22 UTC 2023\n",
      "- Python: 3.9.12\n",
      "- Stable-Baselines3: 2.3.0\n",
      "- PyTorch: 1.13.0+cu116\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 2.2.1\n",
      "- Gymnasium: 0.29.1\n",
      "- OpenAI Gym: 0.26.2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
      "  warnings.warn(\n",
      "c:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = slimevolleygym.SlimeVolleyEnv()\n",
    "models = []\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# PPO Baseline\n",
    "agent = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "agent.load_models(\"Logging/PPO-BASELINE/20240411-150526-lr-0.0003-entcoef-0.1-mlp-64-kl-0.03\", 1, 18492436)\n",
    "models.append({\n",
    "    \"name\": \"PPO - Expert training\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# PPO Selfplay\n",
    "agent = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "agent.load_models(\"Logging/PPO-SELFPLAY/20240410-171658-lr-0.0003-entcoef-0\", 1, 18534177)\n",
    "models.append({\n",
    "    \"name\": \"PPO - Selfplay\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# Genetic agent\n",
    "agent = Model(mlp.games['slimevolleylite'])\n",
    "with open('Logging/GENETIC-SELFPLAY/20240409-021844-numagents-128-totalnumgames-546000/game_546000') as f:\n",
    "    d = json.load(f)\n",
    "    agent.set_model_params(d[0])\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = self.predict(state, mean_mode=greedy)\n",
    "    action = (action > 0).astype(int) # Anything positive means a 1, 0 or negative means a 0\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"Genetic - Selfplay\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# A2C Baseline\n",
    "agent = A2C.load(\"Logging/A2C-BASELINE-LIBRARY/20240416-004821-lr-0.0007-entcoef-0.1/best_model\", env,\\\n",
    "                  print_system_info=True, custom_objects={'observation_space': env.observation_space, 'action_space': env.action_space})\n",
    "def select_action(self, state, greedy=False):\n",
    "    action, _ = self.predict(state, deterministic=greedy)\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"A2C - Expert training\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# A2C Self-play\n",
    "# TODO: Choose the model to load\n",
    "agent = A2C.load(\"Logging/A2C-SELFPLAY-LIBRARY/20240416-192851-lr-0.0007-entcoef-0.1/history_00000080\", env, \\\n",
    "                 print_system_info=True, custom_objects={'observation_space': env.observation_space, 'action_space': env.action_space})\n",
    "def select_action(self, state, greedy=False):\n",
    "    action, _ = self.predict(state, deterministic=greedy)\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"A2C - Selfplay\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# Baseline\n",
    "agent = BaselinePolicy()\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = self.predict(state)\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"Expert baseline\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# Random agent\n",
    "agent = BaselinePolicy()\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = convert_to_value(env.action_space.sample())\n",
    "    return action, None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"Random baseline\",\n",
    "    \"agent\": agent\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Writing a wrapper to evaluate the agents in parallel using sb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SlimeVolleyParallelWrapper(slimevolleygym.SlimeVolleyEnv):\n",
    "  \n",
    "  def __init__(self, opponent_agent):\n",
    "    super(SlimeVolleyParallelWrapper, self).__init__()\n",
    "    opponent_agent.evaluation_mode()\n",
    "    self.opponent_agent = opponent_agent\n",
    "\n",
    "  # We need to override this function with the opponent's policy\n",
    "  # This gym env will then play with that opponent whenever it runs\n",
    "  def predict(self, obs):\n",
    "      action, _ = self.opponent_agent.select_action(obs, greedy=True)\n",
    "      return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then modify the evaluate_policy function from the stable baselines 3 library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Source: https://github.com/DLR-RM/stable-baselines3/blob/master/stable_baselines3/common/evaluation.py\n",
    "# Modified to be able to run our custom agents in a parallelized way (1 environment per CPU core)\n",
    "\n",
    "import warnings\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "\n",
    "from stable_baselines3.common import type_aliases\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecEnv, VecMonitor, is_vecenv_wrapped\n",
    "\n",
    "\n",
    "def evaluate_policy(\n",
    "    model,\n",
    "    env: Union[gym.Env, VecEnv],\n",
    "    n_eval_episodes: int = 10,\n",
    "    render: bool = False,\n",
    "    callback: Optional[Callable[[Dict[str, Any], Dict[str, Any]], None]] = None,\n",
    "    reward_threshold: Optional[float] = None,\n",
    "    return_episode_rewards: bool = False,\n",
    "    warn: bool = True,\n",
    "    combination = None # Modification: For printing the progress\n",
    ") -> Union[Tuple[float, float], Tuple[List[float], List[int]]]:\n",
    "    \"\"\"\n",
    "    Runs policy for ``n_eval_episodes`` episodes and returns average reward.\n",
    "    If a vector env is passed in, this divides the episodes to evaluate onto the\n",
    "    different elements of the vector env. This static division of work is done to\n",
    "    remove bias. See https://github.com/DLR-RM/stable-baselines3/issues/402 for more\n",
    "    details and discussion.\n",
    "\n",
    "    .. note::\n",
    "        If environment has not been wrapped with ``Monitor`` wrapper, reward and\n",
    "        episode lengths are counted as it appears with ``env.step`` calls. If\n",
    "        the environment contains wrappers that modify rewards or episode lengths\n",
    "        (e.g. reward scaling, early episode reset), these will affect the evaluation\n",
    "        results as well. You can avoid this by wrapping environment with ``Monitor``\n",
    "        wrapper before anything else.\n",
    "\n",
    "    :param model: The RL agent you want to evaluate. This can be any object\n",
    "        that implements a `predict` method, such as an RL algorithm (``BaseAlgorithm``)\n",
    "        or policy (``BasePolicy``).\n",
    "    :param env: The gym environment or ``VecEnv`` environment.\n",
    "    :param n_eval_episodes: Number of episode to evaluate the agent\n",
    "    :param deterministic: Whether to use deterministic or stochastic actions\n",
    "    :param render: Whether to render the environment or not\n",
    "    :param callback: callback function to do additional checks,\n",
    "        called after each step. Gets locals() and globals() passed as parameters.\n",
    "    :param reward_threshold: Minimum expected reward per episode,\n",
    "        this will raise an error if the performance is not met\n",
    "    :param return_episode_rewards: If True, a list of rewards and episode lengths\n",
    "        per episode will be returned instead of the mean.\n",
    "    :param warn: If True (default), warns user about lack of a Monitor wrapper in the\n",
    "        evaluation environment.\n",
    "    :return: Mean reward per episode, std of reward per episode.\n",
    "        Returns ([float], [int]) when ``return_episode_rewards`` is True, first\n",
    "        list containing per-episode rewards and second containing per-episode lengths\n",
    "        (in number of steps).\n",
    "    \"\"\"\n",
    "\n",
    "    # Modification: Set the model to evaluation mode\n",
    "    model.evaluation_mode()\n",
    "\n",
    "    is_monitor_wrapped = False\n",
    "    # Avoid circular import\n",
    "    from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "    if not isinstance(env, VecEnv):\n",
    "        env = DummyVecEnv([lambda: env])  # type: ignore[list-item, return-value]\n",
    "\n",
    "    is_monitor_wrapped = is_vecenv_wrapped(env, VecMonitor) or env.env_is_wrapped(Monitor)[0]\n",
    "\n",
    "    if not is_monitor_wrapped and warn:\n",
    "        warnings.warn(\n",
    "            \"Evaluation environment is not wrapped with a ``Monitor`` wrapper. \"\n",
    "            \"This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. \"\n",
    "            \"Consider wrapping environment first with ``Monitor`` wrapper.\",\n",
    "            UserWarning,\n",
    "        )\n",
    "\n",
    "    n_envs = env.num_envs\n",
    "    episode_rewards = []\n",
    "    episode_lengths = []\n",
    "\n",
    "    episode_counts = np.zeros(n_envs, dtype=\"int\")\n",
    "    # Divides episodes among different sub environments in the vector as evenly as possible\n",
    "    episode_count_targets = np.array([(n_eval_episodes + i) // n_envs for i in range(n_envs)], dtype=\"int\")\n",
    "\n",
    "    current_rewards = np.zeros(n_envs)\n",
    "    current_lengths = np.zeros(n_envs, dtype=\"int\")\n",
    "    observations = env.reset()\n",
    "    episode_starts = np.ones((env.num_envs,), dtype=bool)\n",
    "    while (episode_counts < episode_count_targets).any():\n",
    "        actions, _ = model.select_action( # Modification: Use our function header\n",
    "            observations,  # type: ignore[arg-type]\n",
    "            greedy=True\n",
    "        )\n",
    "        actions = convert_list_to_vectors(actions.numpy())\n",
    "        new_observations, rewards, dones, infos = env.step(actions)\n",
    "        current_rewards += rewards\n",
    "        current_lengths += 1\n",
    "        for i in range(n_envs):\n",
    "            if episode_counts[i] < episode_count_targets[i]:\n",
    "                # unpack values so that the callback can access the local variables\n",
    "                reward = rewards[i]\n",
    "                done = dones[i]\n",
    "                info = infos[i]\n",
    "                episode_starts[i] = done\n",
    "\n",
    "                if callback is not None:\n",
    "                    callback(locals(), globals())\n",
    "\n",
    "                if dones[i]:\n",
    "                    if is_monitor_wrapped:\n",
    "                        # Atari wrapper can send a \"done\" signal when\n",
    "                        # the agent loses a life, but it does not correspond\n",
    "                        # to the true end of episode\n",
    "                        if \"episode\" in info.keys():\n",
    "                            # Do not trust \"done\" with episode endings.\n",
    "                            # Monitor wrapper includes \"episode\" key in info if environment\n",
    "                            # has been wrapped with it. Use those rewards instead.\n",
    "                            episode_rewards.append(info[\"episode\"][\"r\"])\n",
    "                            episode_lengths.append(info[\"episode\"][\"l\"])\n",
    "                            # Only increment at the real end of an episode\n",
    "                            episode_counts[i] += 1\n",
    "                            # Modification: Print the progress\n",
    "                            clear_output(wait=True)\n",
    "                            print(f\"Model {combination[0]} VS Model {combination[1]} progress:({episode_counts[0]/episode_count_targets[0]:.2f}%)\")\n",
    "                    else:\n",
    "                        episode_rewards.append(current_rewards[i])\n",
    "                        episode_lengths.append(current_lengths[i])\n",
    "                        episode_counts[i] += 1\n",
    "                    current_rewards[i] = 0\n",
    "                    current_lengths[i] = 0\n",
    "\n",
    "        observations = new_observations\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "    mean_reward = np.mean(episode_rewards)\n",
    "    std_reward = np.std(episode_rewards)\n",
    "    if reward_threshold is not None:\n",
    "        assert mean_reward > reward_threshold, \"Mean reward below threshold: \" f\"{mean_reward:.2f} < {reward_threshold:.2f}\"\n",
    "    if return_episode_rewards:\n",
    "        return episode_rewards, episode_lengths\n",
    "    return mean_reward, std_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Progress:(0.02%)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[51], line 26\u001b[0m\n\u001b[0;32m     23\u001b[0m vec_env \u001b[38;5;241m=\u001b[39m make_vec_env(SlimeVolleyParallelWrapper, n_envs\u001b[38;5;241m=\u001b[39mN_CPU, seed\u001b[38;5;241m=\u001b[39mSEED, env_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopponent_agent\u001b[39m\u001b[38;5;124m\"\u001b[39m: agent2})\n\u001b[0;32m     25\u001b[0m \u001b[38;5;66;03m# Run the evaluations\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m rewards, _ \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_policy\u001b[49m\u001b[43m(\u001b[49m\u001b[43magent1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvec_env\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_eval_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_EVALUATIONS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_episode_rewards\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Convert the rewards to a numpy array\u001b[39;00m\n\u001b[0;32m     29\u001b[0m rewards \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(rewards)\n",
      "Cell \u001b[1;32mIn[50], line 93\u001b[0m, in \u001b[0;36mevaluate_policy\u001b[1;34m(model, env, n_eval_episodes, render, callback, reward_threshold, return_episode_rewards, warn)\u001b[0m\n\u001b[0;32m     91\u001b[0m episode_starts \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mones((env\u001b[38;5;241m.\u001b[39mnum_envs,), dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m (episode_counts \u001b[38;5;241m<\u001b[39m episode_count_targets)\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m---> 93\u001b[0m     actions, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# Modification: Use our function header\u001b[39;49;00m\n\u001b[0;32m     94\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobservations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[0;32m     95\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[0;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     97\u001b[0m     actions \u001b[38;5;241m=\u001b[39m convert_list_to_vectors(actions\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[0;32m     98\u001b[0m     new_observations, rewards, dones, infos \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(actions)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\Models\\PPO\\PPO_Agent.py:241\u001b[0m, in \u001b[0;36mPPO_Agent.select_action\u001b[1;34m(self, obs, greedy)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Query the actor network for a mean action\u001b[39;00m\n\u001b[0;32m    240\u001b[0m obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m--> 241\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Create a categorical distribution over the list of probabilities of actions\u001b[39;00m\n\u001b[0;32m    244\u001b[0m dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\Models\\PPO\\MLP.py:35\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03mForward pass through the network\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_actor:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(state)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "NUM_EVALUATIONS = 1000\n",
    "LOGGING_DIR = \"Logging/EVALUATION\"\n",
    "N_CPU = 50\n",
    "SEED = 32\n",
    "\n",
    "# Make a returns matrix\n",
    "# Last dimension is to store each training episode\n",
    "returns = np.zeros((len(models), len(models), NUM_EVALUATIONS))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    for j in range(i+1, len(models)):\n",
    "\n",
    "        # Extract the models\n",
    "        agent1 = models[i][\"agent\"]\n",
    "        agent2 = models[j][\"agent\"]\n",
    "\n",
    "        # Run the evaluations\n",
    "        # Set the model in evaluation mode\n",
    "        agent1.evaluation_mode()\n",
    "        agent2.evaluation_mode()\n",
    "\n",
    "        # Vectorize the environment\n",
    "        vec_env = make_vec_env(SlimeVolleyParallelWrapper, n_envs=N_CPU, seed=SEED, env_kwargs={\"opponent_agent\": agent2})\n",
    "\n",
    "        # Run the evaluations\n",
    "        rewards, _ = evaluate_policy(agent1, vec_env, n_eval_episodes=NUM_EVALUATIONS, return_episode_rewards=True, combination=(i,j))\n",
    "\n",
    "        # Convert the rewards to a numpy array\n",
    "        rewards = np.array(rewards)\n",
    "        \n",
    "        # Store the returns (The rewards are from the perspective of agent 1)\n",
    "        returns[i, j] = rewards\n",
    "        returns[j, i] = -rewards\n",
    "\n",
    "# Save the returns\n",
    "np.savez(f\"{LOGGING_DIR}/eval_returns.npz\", returns)\n",
    "\n",
    "# Print the pairwise mean returns with standard deviation in table format with the model names as column and row headers\n",
    "mean_returns = np.mean(returns, axis=-1)\n",
    "std_returns = np.std(returns, axis=-1)\n",
    "df = pd.DataFrame(mean_returns, columns=[model[\"name\"] for model in models], index=[model[\"name\"] for model in models])\n",
    "df = df.applymap(lambda x: f\"{x:.2f}\")\n",
    "df += \" +- \"\n",
    "df += pd.DataFrame(std_returns, columns=[model[\"name\"] for model in models], index=[model[\"name\"] for model in models]).applymap(lambda x: f\"{x:.2f}\")\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELO evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the returns\n",
    "# returns = np.load(f\"{LOGGING_DIR}/eval_returns.npz\")[\"arr_0\"]\n",
    "\n",
    "# Compute the new ELOs of both players\n",
    "def calculate_elos(elo1, elo2, s1, s2, K=32):\n",
    "\n",
    "    # Calculate the expected score\n",
    "    expected_score = 1 / (1 + 10**((elo2 - elo1) / 400))\n",
    "\n",
    "    # Calculate the new ELOs\n",
    "    new_elo1 = elo1 + K * (s1 - expected_score)\n",
    "    new_elo2 = elo2 + K * (s2 - (1 - expected_score))\n",
    "\n",
    "    # Return both ELOs\n",
    "    return new_elo1, new_elo2\n",
    "\n",
    "# Initialize the ELOs at 12000\n",
    "elos = np.zeros(len(models))\n",
    "\n",
    "# Extract the array of (agent1, agent2, s1, s2)\n",
    "# We do this to avoid replaying all the episodes\n",
    "k = 0\n",
    "games = np.zeros((len(models) * len(models) * NUM_EVALUATIONS, 4))\n",
    "for i in range(len(models)):\n",
    "    for j in range(i+1, len(models)):\n",
    "        for e in range(NUM_EVALUATIONS):\n",
    "            if returns[i, j, e] > 0:\n",
    "                games[k] = np.array([i, j, 1, 0])\n",
    "            elif returns[i, j, e] < 0:\n",
    "                games[k] = np.array([i, j, 0, 1])\n",
    "            else:\n",
    "                games[k] = np.array([i, j, 0.5, 0.5])\n",
    "            k += 1\n",
    "\n",
    "# Shuffle the list of games and update the elos based on the results\n",
    "np.random.shuffle(games)\n",
    "for i, j, s1, s2 in games:\n",
    "    elos[i], elos[j] = calculate_elos(elos[i], elos[j], s1, s2)\n",
    "\n",
    "# Save the ELOs\n",
    "np.savez(f\"{LOGGING_DIR}/eval_elos.npz\", elos)\n",
    "\n",
    "# Print the ELOs\n",
    "print(\"ELO computation complete!\")\n",
    "for i, model in enumerate(models):\n",
    "    print(f\"{model['name']} ELO: {elos[i]}\")\n",
    "\n",
    "# Load the ELOs\n",
    "# elos = np.load(f\"{LOGGING_DIR}/eval_elos.npz\")[\"arr_0\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
