{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.PPO.PPO_Agent import PPO_Agent\n",
    "from Models.DDQN.DDQN_Agent import DDQN_Agent\n",
    "from Models.DDQN.PRB import PrioritizedReplayBuffer\n",
    "from stable_baselines3 import A2C\n",
    "import slimevolleygym.mlp as mlp\n",
    "from slimevolleygym.mlp import Model\n",
    "import torch\n",
    "import slimevolleygym\n",
    "from slimevolleygym import BaselinePolicy\n",
    "from utils import convert_to_vector, convert_to_value\n",
    "import types\n",
    "import json\n",
    "import numpy as np\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Print the device as a check\n",
    "print(\"Device used: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the models as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== CURRENT SYSTEM INFO ==\n",
      "- OS: Windows-10-10.0.22631-SP0 10.0.22631\n",
      "- Python: 3.10.4\n",
      "- Stable-Baselines3: 2.3.0\n",
      "- PyTorch: 2.2.2+cu118\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.23.1\n",
      "- Cloudpickle: 3.0.0\n",
      "- Gymnasium: 0.29.1\n",
      "- OpenAI Gym: 0.21.0\n",
      "\n",
      "== SAVED MODEL SYSTEM INFO ==\n",
      "- OS: Linux-5.4.0-144-generic-x86_64-with-glibc2.27 # 161~18.04.1-Ubuntu SMP Fri Feb 10 15:55:22 UTC 2023\n",
      "- Python: 3.9.12\n",
      "- Stable-Baselines3: 2.3.0\n",
      "- PyTorch: 1.13.0+cu116\n",
      "- GPU Enabled: True\n",
      "- Numpy: 1.26.4\n",
      "- Cloudpickle: 2.2.1\n",
      "- Gymnasium: 0.29.1\n",
      "- OpenAI Gym: 0.26.2\n",
      "\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:95: UserWarning: You loaded a model that was trained using OpenAI Gym. We strongly recommend transitioning to Gymnasium by saving that model again.\n",
      "  warnings.warn(\n",
      "c:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\stable_baselines3\\common\\vec_env\\patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "env = slimevolleygym.SlimeVolleyEnv()\n",
    "models = []\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# PPO Baseline\n",
    "agent = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "agent.load_models(\"Logging/PPO-BASELINE/20240411-150526-lr-0.0003-entcoef-0.1-mlp-64-kl-0.03\", 1, 18492436)\n",
    "models.append({\n",
    "    \"name\": \"PPO - Expert training\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# PPO Selfplay\n",
    "agent = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "agent.load_models(\"Logging/PPO-SELFPLAY/20240410-171658-lr-0.0003-entcoef-0\", 1, 18534177)\n",
    "models.append({\n",
    "    \"name\": \"PPO - Selfplay training\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# Genetic agent\n",
    "agent = Model(mlp.games['slimevolleylite'])\n",
    "with open('Logging/GENETIC-SELFPLAY/20240409-021844-numagents-128-totalnumgames-546000/game_546000') as f:\n",
    "    d = json.load(f)\n",
    "    agent.set_model_params(d[0])\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = self.predict(state, mean_mode=greedy)\n",
    "    action = (action > 0).astype(int) # Anything positive means a 1, 0 or negative means a 0\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"Genetic - Selfplay training\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# A2C Baseline\n",
    "# TODO: Choose the model to load\n",
    "agent = A2C.load(\"Logging/A2C-BASELINE-LIBRARY/20240415-184854-lr-0.0007-entcoef-0/best_model\", env,\\\n",
    "                  print_system_info=True, custom_objects={'observation_space': env.observation_space, 'action_space': env.action_space})\n",
    "def select_action(self, state, greedy=False):\n",
    "    action, _ = self.predict(state, deterministic=greedy)\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"A2C - Expert training\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# A2C Self-play\n",
    "# TODO: Choose the model to load\n",
    "# agent = A2C.load(\"Logging/A2C-BASELINE-LIBRARY/20240415-184854-lr-0.0007-entcoef-0/final_model\", print_system_info=True)\n",
    "# def select_action(self, state, greedy=False):\n",
    "#     action, _ = self.predict(state, deterministic=greedy)\n",
    "#     return convert_to_value(action), None\n",
    "# def evaluation_mode(self):\n",
    "#     pass\n",
    "# agent.select_action = types.MethodType(select_action, agent)\n",
    "# agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "# models.append({\n",
    "#     \"name\": \"A2C - Selfplay training\",\n",
    "#     \"agent\": agent\n",
    "# })\n",
    "\n",
    "# DDQN Baseline\n",
    "# TODO: Choose the model to load\n",
    "# buffer = PrioritizedReplayBuffer(\n",
    "#         buffer_size = 1, \n",
    "#         state_dim = 1, \n",
    "#         alpha = 1, \n",
    "#         beta_init = 1, \n",
    "#         device = DEVICE\n",
    "#     )\n",
    "# agent = DDQN_Agent(state_dim = 12,\n",
    "#                     action_dim = 6,\n",
    "#                     hidden_layer_shape = 256,\n",
    "#                     device = DEVICE,\n",
    "#                     lr = 0,\n",
    "#                     gamma = 0,\n",
    "#                     batch_size = 0,\n",
    "#                     epsilon = 0)\n",
    "# agent.load(\"Logging/DDQN-BASELINE/20240414-052734-lr-0.0005\", 1, 10589200, buffer)\n",
    "# agent.select_action_original = agent.select_action\n",
    "# def select_action(self, state, greedy=False, deterministic=False):\n",
    "#     action = self.select_action_original(state, deterministic=greedy)\n",
    "#     return action, None\n",
    "# agent.select_action = types.MethodType(select_action, agent)\n",
    "# models.append({\n",
    "#     \"name\": \"DDQN - Expert training\",\n",
    "#     \"agent\": agent\n",
    "# })\n",
    "\n",
    "# DDQN Self-play\n",
    "# TODO: Choose the model to load\n",
    "# buffer = PrioritizedReplayBuffer(\n",
    "#         buffer_size = 1, \n",
    "#         state_dim = 1, \n",
    "#         alpha = 1, \n",
    "#         beta_init = 1, \n",
    "#         device = DEVICE\n",
    "#     )\n",
    "# agent = DDQN_Agent(state_dim = 12,\n",
    "#                     action_dim = 6,\n",
    "#                     hidden_layer_shape = 256,\n",
    "#                     device = DEVICE,\n",
    "#                     lr = 0,\n",
    "#                     gamma = 0,\n",
    "#                     batch_size = 0,\n",
    "#                     epsilon = 0)\n",
    "# agent.load(\"Logging/DDQN-BASELINE/20240414-052734-lr-0.0005\", 1, 10589200, buffer)\n",
    "# agent.select_action_original = agent.select_action\n",
    "# def select_action(self, state, greedy=False, deterministic=False):\n",
    "#     action = self.select_action_original(state, deterministic=greedy)\n",
    "#     return action, None\n",
    "# agent.select_action = types.MethodType(select_action, agent)\n",
    "# models.append({\n",
    "#     \"name\": \"DDQN - Selfplay training\",\n",
    "#     \"agent\": agent\n",
    "# })\n",
    "\n",
    "# Baseline\n",
    "agent = BaselinePolicy()\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = self.predict(state)\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"Expert baseline\",\n",
    "    \"agent\": agent\n",
    "})\n",
    "\n",
    "# Random agent\n",
    "agent = BaselinePolicy()\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = convert_to_value(env.action_space.sample())\n",
    "    return action, None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent.select_action = types.MethodType(select_action, agent)\n",
    "agent.evaluation_mode = types.MethodType(evaluation_mode, agent)\n",
    "models.append({\n",
    "    \"name\": \"Random baseline\",\n",
    "    \"agent\": agent\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EVALUATIONS = 1000\n",
    "LOGGING_DIR = \"Logging/EVALUATION\"\n",
    "\n",
    "# Make a returns matrix\n",
    "# Last dimension is to store each training episode\n",
    "returns = np.zeros((len(models), len(models), NUM_EVALUATIONS))\n",
    "\n",
    "for i in range(len(models)):\n",
    "    for j in range(i+1, len(models)):\n",
    "\n",
    "        # Extract the models\n",
    "        agent1 = models[i][\"agent\"]\n",
    "        agent2 = models[j][\"agent\"]\n",
    "\n",
    "        # Run the evaluations\n",
    "        # Set the model in evaluation mode\n",
    "        agent1.evaluation_mode()\n",
    "        agent2.evaluation_mode()\n",
    "        \n",
    "        # Run num_eval_episodes episodes and calculate the total return\n",
    "        for e in tqdm(range(NUM_EVALUATIONS)):\n",
    "            \n",
    "            # Initialize the variables\n",
    "            state1 = env.reset()\n",
    "            state2 = state1\n",
    "            done = False\n",
    "            total_return = 0\n",
    "\n",
    "            while not done:\n",
    "                \n",
    "                # Select the actions for each agent\n",
    "                with torch.no_grad():\n",
    "                    action1, _ = agent1.select_action(state1, greedy=True)\n",
    "                    action2, _ = agent2.select_action(state2, greedy=True)\n",
    "                \n",
    "                # Step the environment forward\n",
    "                next_state1, reward, done, info = env.step(convert_to_vector(action1), otherAction=convert_to_vector(action2))\n",
    "                next_state2 = info['otherObs']\n",
    "                \n",
    "                # Add the individual agents' rewards to the total returns (Since they're the same for both agents)\n",
    "                total_return += reward\n",
    "\n",
    "                # Update the states\n",
    "                state1 = next_state1\n",
    "                state2 = next_state2\n",
    "        \n",
    "            # Store the returns\n",
    "            returns[i, j, e] = total_return\n",
    "            returns[j, i, e] = -total_return\n",
    "\n",
    "# Save the returns\n",
    "np.savez(f\"{LOGGING_DIR}/eval_returns.npz\", returns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ELO evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the returns\n",
    "# returns = np.load(f\"{LOGGING_DIR}/eval_returns.npz\")[\"arr_0\"]\n",
    "\n",
    "# Compute the new ELOs of both players\n",
    "def calculate_elos(elo1, elo2, s1, s2, K=32):\n",
    "\n",
    "    # Calculate the expected score\n",
    "    expected_score = 1 / (1 + 10**((elo2 - elo1) / 400))\n",
    "\n",
    "    # Calculate the new ELOs\n",
    "    new_elo1 = elo1 + K * (s1 - expected_score)\n",
    "    new_elo2 = elo2 + K * (s2 - (1 - expected_score))\n",
    "\n",
    "    # Return both ELOs\n",
    "    return new_elo1, new_elo2\n",
    "\n",
    "# Initialize the ELOs at 12000\n",
    "elos = np.zeros(len(models))\n",
    "\n",
    "# Extract the array of (agent1, agent2, s1, s2)\n",
    "# We do this to avoid replaying all the episodes\n",
    "k = 0\n",
    "games = np.zeros((len(models) * len(models) * NUM_EVALUATIONS, 4))\n",
    "for i in range(len(models)):\n",
    "    for j in range(i+1, len(models)):\n",
    "        for e in range(NUM_EVALUATIONS):\n",
    "            if returns[i, j, e] > 0:\n",
    "                games[k] = np.array([i, j, 1, 0])\n",
    "            elif returns[i, j, e] < 0:\n",
    "                games[k] = np.array([i, j, 0, 1])\n",
    "            else:\n",
    "                games[k] = np.array([i, j, 0.5, 0.5])\n",
    "            k += 1\n",
    "\n",
    "# Shuffle the list of games and update the elos based on the results\n",
    "np.random.shuffle(games)\n",
    "for i, j, s1, s2 in games:\n",
    "    elos[i], elos[j] = calculate_elos(elos[i], elos[j], s1, s2)\n",
    "\n",
    "# Save the ELOs\n",
    "np.savez(f\"{LOGGING_DIR}/eval_elos.npz\", elos)\n",
    "\n",
    "# Load the ELOs\n",
    "# elos = np.load(f\"{LOGGING_DIR}/eval_elos.npz\")[\"arr_0\"]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
