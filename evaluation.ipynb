{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.PPO.PPO_Agent import PPO_Agent\n",
    "from Models.DDQN.DDQN_Agent import DDQN_Agent\n",
    "from Models.DDQN.PRB import PrioritizedReplayBuffer\n",
    "from stable_baselines3 import A2C\n",
    "import slimevolleygym.mlp as mlp\n",
    "from slimevolleygym.mlp import Model\n",
    "import torch\n",
    "import slimevolleygym\n",
    "from slimevolleygym import BaselinePolicy\n",
    "from utils import convert_to_vector, convert_to_value\n",
    "import types\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Print the device as a check\n",
    "print(\"Device used: \", DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load all the models as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = slimevolleygym.SlimeVolleyEnv()\n",
    "models = []\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# PPO Baseline\n",
    "agent = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "agent.load_models(\"Logging/PPO-BASELINE/20240411-150526-lr-0.0003-entcoef-0.1-mlp-64-kl-0.03\", 1, 18492436)\n",
    "\n",
    "# PPO Selfplay\n",
    "agent1 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "agent1.load_models(\"Logging/PPO-SELFPLAY/20240410-171658-lr-0.0003-entcoef-0\", 1, 18534177)\n",
    "\n",
    "# Genetic agent\n",
    "agent1 = Model(mlp.games['slimevolleylite'])\n",
    "with open('Logging/GENETIC-SELFPLAY/20240409-021844-numagents-128-totalnumgames-546000/game_546000') as f:\n",
    "    d = json.load(f)\n",
    "    agent1.set_model_params(d[0])\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = self.predict(state, mean_mode=greedy)\n",
    "    action = (action > 0).astype(int) # Anything positive means a 1, 0 or negative means a 0\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent1.select_action = types.MethodType(select_action, agent1)\n",
    "agent1.evaluation_mode = types.MethodType(evaluation_mode, agent1)\n",
    "\n",
    "# A2C Baseline\n",
    "model = A2C.load(\"Logging/A2C-BASELINE-LIBRARY/20240414-154203-lr-0.0003-entcoef-0.1-mlp-64\", print_system_info=True)\n",
    "\n",
    "# DDQN Baseline\n",
    "# buffer = PrioritizedReplayBuffer(\n",
    "#         buffer_size = 1, \n",
    "#         state_dim = 1, \n",
    "#         alpha = 1, \n",
    "#         beta_init = 1, \n",
    "#         device = DEVICE\n",
    "#     )\n",
    "# agent1 = DDQN_Agent(state_dim = 12,\n",
    "#                     action_dim = 6,\n",
    "#                     hidden_layer_shape = 256,\n",
    "#                     device = DEVICE,\n",
    "#                     lr = 0,\n",
    "#                     gamma = 0,\n",
    "#                     batch_size = 0,\n",
    "#                     epsilon = 0)\n",
    "# agent1.load(\"Logging/DDQN-BASELINE/20240414-052734-lr-0.0005\", 1, 10589200, buffer)\n",
    "# agent1.select_action_original = agent1.select_action\n",
    "# def select_action(self, state, greedy=False, deterministic=False):\n",
    "#     action = self.select_action_original(state, deterministic=greedy)\n",
    "#     return action, None\n",
    "# agent1.select_action = types.MethodType(select_action, agent1)\n",
    "\n",
    "# DDQN Self-play\n",
    "# buffer = PrioritizedReplayBuffer(\n",
    "#         buffer_size = 1, \n",
    "#         state_dim = 1, \n",
    "#         alpha = 1, \n",
    "#         beta_init = 1, \n",
    "#         device = DEVICE\n",
    "#     )\n",
    "# agent1 = DDQN_Agent(state_dim = 12,\n",
    "#                     action_dim = 6,\n",
    "#                     hidden_layer_shape = 256,\n",
    "#                     device = DEVICE,\n",
    "#                     lr = 0,\n",
    "#                     gamma = 0,\n",
    "#                     batch_size = 0,\n",
    "#                     epsilon = 0)\n",
    "# agent1.load(\"Logging/DDQN-BASELINE/20240414-052734-lr-0.0005\", 1, 10589200, buffer)\n",
    "# agent1.select_action_original = agent1.select_action\n",
    "# def select_action(self, state, greedy=False, deterministic=False):\n",
    "#     action = self.select_action_original(state, deterministic=greedy)\n",
    "#     return action, None\n",
    "# agent1.select_action = types.MethodType(select_action, agent1)\n",
    "\n",
    "# Baseline\n",
    "agent2 = BaselinePolicy()\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = self.predict(state)\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent2.select_action = types.MethodType(select_action, agent2)\n",
    "agent2.evaluation_mode = types.MethodType(evaluation_mode, agent2)\n",
    "\n",
    "# Random agent\n",
    "agent2 = BaselinePolicy()\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = convert_to_value(env.action_space.sample())\n",
    "    return action, None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent2.select_action = types.MethodType(select_action, agent2)\n",
    "agent2.evaluation_mode = types.MethodType(evaluation_mode, agent2)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
