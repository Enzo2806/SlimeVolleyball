{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Models.PPO.PPO_Agent import PPO_Agent\n",
    "import torch\n",
    "import slimevolleygym\n",
    "from utils import convert_to_vector, convert_to_value\n",
    "import types\n",
    "import slimevolleygym\n",
    "import slimevolleygym.mlp as mlp\n",
    "from slimevolleygym.mlp import Model\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device used:  cuda:0\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "\n",
    "# Print the device as a check\n",
    "print(\"Device used: \", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = slimevolleygym.SlimeVolleyEnv()\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#\n",
    "# Pick one of the baselines below to use as the first agent\n",
    "#\n",
    "\n",
    "# # 1a- PPO Baseline\n",
    "# agent1 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "# agent1.load_models(\"Logging/PPO-BASELINE/20240410-091619-lr-0.0003-entcoef-0-mlp-64-kl-0.03\", 1, 20003331)\n",
    "\n",
    "# 1b- PPO Selfplay\n",
    "agent1 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "agent1.load_models(\"Logging/PPO-SELFPLAY/20240410-171658-lr-0.0003-entcoef-0\", 1, 20004667)\n",
    "\n",
    "# # 1c- DDQN Baseline\n",
    "# buffer = PrioritizedReplayBuffer(\n",
    "#         buffer_size = 1, \n",
    "#         state_dim = 1, \n",
    "#         alpha = 1, \n",
    "#         beta_init = 1, \n",
    "#         device = DEVICE\n",
    "#     )\n",
    "# agent1 = DDQN_Agent(state_dim = 12,\n",
    "#                     action_dim = 6,\n",
    "#                     hidden_layer_shape = 256,\n",
    "#                     device = DEVICE,\n",
    "#                     lr = 0,\n",
    "#                     gamma = 0,\n",
    "#                     batch_size = 0,\n",
    "#                     epsilon = 0)\n",
    "# agent1.load(\"Logging/DDQN-BASELINE/20240414-052734-lr-0.0005\", 1, 10589200, buffer)\n",
    "# agent1.select_action_original = agent1.select_action\n",
    "# def select_action(self, state, greedy=False, deterministic=False):\n",
    "#     action = self.select_action_original(state, deterministic=greedy)\n",
    "#     return action, None\n",
    "# agent1.select_action = types.MethodType(select_action, agent1)\n",
    "\n",
    "# # 2- Genetic agent\n",
    "# agent1 = Model(mlp.games['slimevolleylite'])\n",
    "# with open('Logging/GENETIC-SELFPLAY/20240409-021844-numagents-128-totalnumgames-546000/game_546000') as f:\n",
    "#     d = json.load(f)\n",
    "#     agent1.set_model_params(d[0])\n",
    "# def select_action(self, state, greedy=False):\n",
    "#     action = self.predict(state, mean_mode=greedy)\n",
    "#     action = (action > 0).astype(int) # Anything positive means a 1, 0 or negative means a 0\n",
    "#     return convert_to_value(action), None\n",
    "# def evaluation_mode(self):\n",
    "#     pass\n",
    "# agent1.select_action = types.MethodType(select_action, agent1)\n",
    "# agent1.evaluation_mode = types.MethodType(evaluation_mode, agent1)\n",
    "\n",
    "#\n",
    "# Pick one of the snippets below to use as the second agent\n",
    "#\n",
    "\n",
    "# # 1a- PPO Baseline\n",
    "# agent1 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "# agent1.load_models(\"Logging/PPO-BASELINE/20240410-091619-lr-0.0003-entcoef-0-mlp-64-kl-0.03.pt\", 1, 20003331)\n",
    "\n",
    "# # 1b- PPO Selfplay\n",
    "# agent2 = PPO_Agent(12, 6, DEVICE, mlp_layers=[64, 64])\n",
    "# agent2.load_models(\"Logging/PPO-SELFPLAY/20240410-171658-lr-0.0003-entcoef-0\", 1, 20004667)\n",
    "\n",
    "# # 2- Baseline\n",
    "# agent2 = BaselinePolicy()\n",
    "# def select_action(self, state, greedy=False):\n",
    "#     action = self.predict(state)\n",
    "#     return convert_to_value(action), None\n",
    "# def evaluation_mode(self):\n",
    "#     pass\n",
    "# agent2.select_action = types.MethodType(select_action, agent2)\n",
    "# agent2.evaluation_mode = types.MethodType(evaluation_mode, agent2)\n",
    "\n",
    "# # 3- Random agent\n",
    "# agent2 = BaselinePolicy()\n",
    "# def select_action(self, state, greedy=False):\n",
    "#     action = convert_to_value(env.action_space.sample())\n",
    "#     return action, None\n",
    "# def evaluation_mode(self):\n",
    "#     pass\n",
    "# agent2.select_action = types.MethodType(select_action, agent2)\n",
    "# agent2.evaluation_mode = types.MethodType(evaluation_mode, agent2)\n",
    "\n",
    "# 4- Genetic agent\n",
    "agent2 = Model(mlp.games['slimevolleylite'])\n",
    "with open('Logging/GENETIC-SELFPLAY/20240409-021844-numagents-128-totalnumgames-546000/game_546000') as f:\n",
    "    d = json.load(f)\n",
    "    agent2.set_model_params(d[0])\n",
    "def select_action(self, state, greedy=False):\n",
    "    action = self.predict(state, mean_mode=greedy)\n",
    "    action = (action > 0).astype(int) # Anything positive means a 1, 0 or negative means a 0\n",
    "    return convert_to_value(action), None\n",
    "def evaluation_mode(self):\n",
    "    pass\n",
    "agent2.select_action = types.MethodType(select_action, agent2)\n",
    "agent2.evaluation_mode = types.MethodType(evaluation_mode, agent2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m\n\u001b[0;32m     15\u001b[0m env\u001b[38;5;241m.\u001b[39mrender(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhuman\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     18\u001b[0m \n\u001b[0;32m     19\u001b[0m     \u001b[38;5;66;03m# Select the actions for each agent\u001b[39;00m\n\u001b[1;32m---> 20\u001b[0m     action1, _ \u001b[38;5;241m=\u001b[39m \u001b[43magent1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgreedy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     21\u001b[0m     action2, _ \u001b[38;5;241m=\u001b[39m agent2\u001b[38;5;241m.\u001b[39mselect_action(state2, greedy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;66;03m# Step the environment forward\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\Models\\PPO\\PPO_Agent.py:241\u001b[0m, in \u001b[0;36mPPO_Agent.select_action\u001b[1;34m(self, obs, greedy)\u001b[0m\n\u001b[0;32m    239\u001b[0m \u001b[38;5;66;03m# Query the actor network for a mean action\u001b[39;00m\n\u001b[0;32m    240\u001b[0m obs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(obs,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mDEVICE)\n\u001b[1;32m--> 241\u001b[0m probs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    243\u001b[0m \u001b[38;5;66;03m# Create a categorical distribution over the list of probabilities of actions\u001b[39;00m\n\u001b[0;32m    244\u001b[0m dist \u001b[38;5;241m=\u001b[39m Categorical(probs)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\Models\\PPO\\MLP.py:35\u001b[0m, in \u001b[0;36mMLP.forward\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;124;03mForward pass through the network\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_actor:\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39msoftmax(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnetwork(state)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\modules\\activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\wassi\\projects\\gitrepos\\SlimeVolleyball\\env\\lib\\site-packages\\torch\\nn\\functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[1;34m(input, inplace)\u001b[0m\n\u001b[0;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[0;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Set the agents to evaluation mode\n",
    "agent1.evaluation_mode()\n",
    "agent2.evaluation_mode()\n",
    "\n",
    "# Run num_eval_episodes episodes and calculate the total return\n",
    "total_return = 0\n",
    "for _ in range(1):\n",
    "\n",
    "    state1 = env.reset()\n",
    "    state2 = state1\n",
    "    done = False\n",
    "    while not done:\n",
    "\n",
    "        # Render the environment\n",
    "        env.render(mode=\"human\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "\n",
    "            # Select the actions for each agent\n",
    "            action1, _ = agent1.select_action(state1, greedy=True)\n",
    "            action2, _ = agent2.select_action(state2, greedy=True)\n",
    "        \n",
    "        # Step the environment forward\n",
    "        next_state1, reward, done, info = env.step(convert_to_vector(action1), otherAction=convert_to_vector(action2))\n",
    "        next_state2 = info['otherObs']\n",
    "        \n",
    "        # Add the individual agents' rewards to the total returns (Since they're the same for both agents)\n",
    "        total_return += reward\n",
    "\n",
    "        # Update the states\n",
    "        state1 = next_state1\n",
    "        state2 = next_state2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
