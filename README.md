# SlimeVolleyball: A comparative analysis of different training algorithms for 1v1 RL Environments

## Overview
This project explored the two primary methods for training 1v1 agents—expert training and self-play—within the SlimeVolleyball gym environment. We used various algorithms such as PPO, DDQN, A2C, and a genetic approach. Agents were evaluated and ranked based on their ELO scores, revealing that self-play yields more adaptable agents. PPO emerged as the most straightforward algorithm for training among the RL agents tested. Our study also documented the evolution of strategies during self-play, providing insights into how agents learn and optimize their strategies over time.

## Full Report & Summary Video
For a comprehensive explanation of the techniques employed and the metrics recorded, refer to the full report shown below. A video summary of the project can be viewed here: 
[![Summary Video](https://github.com/user-attachments/assets/90359fcf-6cce-495e-941c-9962f21786ab)](https://drive.google.com/file/d/1_mM4yksDoHHEKkiA3Q6f8YquGqjig114/view?usp=sharing)


![Report_Page_1](https://github.com/user-attachments/assets/d3c1f753-cefa-4602-8b7b-d816884e292d)
![Report_Page_2](https://github.com/user-attachments/assets/e7204311-0d5f-4311-9e61-d798e9e65741)
![Report_Page_3](https://github.com/user-attachments/assets/718a9403-4e6c-4dfb-a6ba-9410c8553cbb)
![Report_Page_4](https://github.com/user-attachments/assets/d592209b-02cc-4013-800f-d807c5be09d6)
![Report_Page_5](https://github.com/user-attachments/assets/bf7ae5e8-eeb3-4ff5-801d-30af24e1bf2d)
![Report_Page_6](https://github.com/user-attachments/assets/da186564-6319-490f-b1dc-38629a344e8a)
![Report_Page_7](https://github.com/user-attachments/assets/cb56b881-92c4-4142-9157-271b0f851f84)
![Report_Page_8](https://github.com/user-attachments/assets/98d383cd-ad92-4606-966f-cb1eb06f49f9)
