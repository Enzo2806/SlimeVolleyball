# SlimeVolleyball

## Overview
  This study consisted of exploring the two main methods for training 1v1 agents, expert training and selfplay, on the SlimeVolleyball gym environment. The algorithms implemented are PPO, DDQN, A2C, and a genetic approach. By ranking the agents using their ELO scores, we are able to conclude that selfplay leads to more flexible agents, and that PPO is the easiest to train out of the RL agents. We also observed the evolution of the selfplay strategies across generations to better understand the way agents learn an optimal strategy.

## Full report & summarizing video
  An in-depth report explaining the techniques attempted and reporting interesting metrics can be found [here](https://drive.google.com/file/d/1_mEP8LTdSd5vENL6gEheh9squj83-1th/view?usp=sharing). A summary of the project is provided in the following video: [![Video](https://github.com/user-attachments/assets/90359fcf-6cce-495e-941c-9962f21786ab)](https://drive.google.com/file/d/1_mM4yksDoHHEKkiA3Q6f8YquGqjig114/view?usp=sharing)
