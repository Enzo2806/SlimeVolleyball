{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired from: https://github.com/hardmaru/slimevolleygym/blob/master/training_scripts/train_ppo.py\n",
    "import os\n",
    "import gym\n",
    "import slimevolleygym\n",
    "from datetime import datetime\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "import torch\n",
    "from stable_baselines3.common.env_util import make_vec_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_TIMESTEPS = int(5e7)\n",
    "SEED = 721\n",
    "EVAL_EPISODES = 100\n",
    "n_cpu = 25\n",
    "EVAL_FREQ = 500000 // n_cpu\n",
    "learning_rate=0.0007\n",
    "n_steps=5\n",
    "gamma=0.99\n",
    "gae_lambda=1.0\n",
    "ent_coef=0.1\n",
    "vf_coef=0.5\n",
    "max_grad_norm=0.5\n",
    "rms_prop_eps=1e-05\n",
    "use_rms_prop=True\n",
    "use_sde=False\n",
    "sde_sample_freq=-1\n",
    "rollout_buffer_class=None\n",
    "rollout_buffer_kwargs=None\n",
    "normalize_advantage=False\n",
    "stats_window_size=100\n",
    "policy_kwargs=None\n",
    "verbose=1\n",
    "device=torch.device('cuda:0' if torch.cuda.is_available() else 'mps' if torch.backends.mps.is_available() else 'cpu')\n",
    "_init_setup_model=True\n",
    "\n",
    "# Log dir\n",
    "LOGDIR = f\"./Logging/A2C-BASELINE-LIBRARY/{datetime.now().strftime('%Y%m%d-%H%M%S')}-lr-{learning_rate}-entcoef-{ent_coef}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda:0 device\n",
      "Logging to ./Logging/A2C-BASELINE-LIBRARY/20240415-225115-lr-0.0007-entcoef-0.1/A2C_1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 449      |\n",
      "|    ep_rew_mean        | -5       |\n",
      "| time/                 |          |\n",
      "|    fps                | 6762     |\n",
      "|    iterations         | 100      |\n",
      "|    time_elapsed       | 3        |\n",
      "|    total_timesteps    | 25000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.07    |\n",
      "|    explained_variance | 0.909    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 99       |\n",
      "|    policy_loss        | 0.000582 |\n",
      "|    value_loss         | 0.0055   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 575      |\n",
      "|    ep_rew_mean        | -4.97    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6739     |\n",
      "|    iterations         | 200      |\n",
      "|    time_elapsed       | 7        |\n",
      "|    total_timesteps    | 50000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.736    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 199      |\n",
      "|    policy_loss        | -0.0522  |\n",
      "|    value_loss         | 0.0338   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 574      |\n",
      "|    ep_rew_mean        | -4.96    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6734     |\n",
      "|    iterations         | 300      |\n",
      "|    time_elapsed       | 11       |\n",
      "|    total_timesteps    | 75000    |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.828    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 299      |\n",
      "|    policy_loss        | 0.0133   |\n",
      "|    value_loss         | 0.017    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 585      |\n",
      "|    ep_rew_mean        | -4.94    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6728     |\n",
      "|    iterations         | 400      |\n",
      "|    time_elapsed       | 14       |\n",
      "|    total_timesteps    | 100000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.859    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 399      |\n",
      "|    policy_loss        | 0.0116   |\n",
      "|    value_loss         | 0.0136   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 609      |\n",
      "|    ep_rew_mean        | -4.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 6727     |\n",
      "|    iterations         | 500      |\n",
      "|    time_elapsed       | 18       |\n",
      "|    total_timesteps    | 125000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.928    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 499      |\n",
      "|    policy_loss        | -0.00252 |\n",
      "|    value_loss         | 0.00487  |\n",
      "------------------------------------\n",
      "-------------------------------------\n",
      "| rollout/              |           |\n",
      "|    ep_len_mean        | 602       |\n",
      "|    ep_rew_mean        | -4.91     |\n",
      "| time/                 |           |\n",
      "|    fps                | 6725      |\n",
      "|    iterations         | 600       |\n",
      "|    time_elapsed       | 22        |\n",
      "|    total_timesteps    | 150000    |\n",
      "| train/                |           |\n",
      "|    entropy_loss       | -2.08     |\n",
      "|    explained_variance | 0.865     |\n",
      "|    learning_rate      | 0.0007    |\n",
      "|    n_updates          | 599       |\n",
      "|    policy_loss        | -0.000232 |\n",
      "|    value_loss         | 0.0111    |\n",
      "-------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 601      |\n",
      "|    ep_rew_mean        | -4.94    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6722     |\n",
      "|    iterations         | 700      |\n",
      "|    time_elapsed       | 26       |\n",
      "|    total_timesteps    | 175000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.886    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 699      |\n",
      "|    policy_loss        | 0.00816  |\n",
      "|    value_loss         | 0.0155   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 598      |\n",
      "|    ep_rew_mean        | -4.94    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6721     |\n",
      "|    iterations         | 800      |\n",
      "|    time_elapsed       | 29       |\n",
      "|    total_timesteps    | 200000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.833    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 799      |\n",
      "|    policy_loss        | 0.00311  |\n",
      "|    value_loss         | 0.015    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 610      |\n",
      "|    ep_rew_mean        | -4.89    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6720     |\n",
      "|    iterations         | 900      |\n",
      "|    time_elapsed       | 33       |\n",
      "|    total_timesteps    | 225000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 899      |\n",
      "|    policy_loss        | 0.0103   |\n",
      "|    value_loss         | 0.00643  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 619      |\n",
      "|    ep_rew_mean        | -4.8     |\n",
      "| time/                 |          |\n",
      "|    fps                | 6720     |\n",
      "|    iterations         | 1000     |\n",
      "|    time_elapsed       | 37       |\n",
      "|    total_timesteps    | 250000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.752    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 999      |\n",
      "|    policy_loss        | 0.0579   |\n",
      "|    value_loss         | 0.0219   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 605      |\n",
      "|    ep_rew_mean        | -4.89    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6720     |\n",
      "|    iterations         | 1100     |\n",
      "|    time_elapsed       | 40       |\n",
      "|    total_timesteps    | 275000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.841    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1099     |\n",
      "|    policy_loss        | -0.0351  |\n",
      "|    value_loss         | 0.0236   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 594      |\n",
      "|    ep_rew_mean        | -4.87    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6719     |\n",
      "|    iterations         | 1200     |\n",
      "|    time_elapsed       | 44       |\n",
      "|    total_timesteps    | 300000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.746    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1199     |\n",
      "|    policy_loss        | -0.0224  |\n",
      "|    value_loss         | 0.0275   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 601      |\n",
      "|    ep_rew_mean        | -4.87    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6719     |\n",
      "|    iterations         | 1300     |\n",
      "|    time_elapsed       | 48       |\n",
      "|    total_timesteps    | 325000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.936    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1299     |\n",
      "|    policy_loss        | 0.00503  |\n",
      "|    value_loss         | 0.00472  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 614      |\n",
      "|    ep_rew_mean        | -4.81    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6719     |\n",
      "|    iterations         | 1400     |\n",
      "|    time_elapsed       | 52       |\n",
      "|    total_timesteps    | 350000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.835    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1399     |\n",
      "|    policy_loss        | 0.0263   |\n",
      "|    value_loss         | 0.0229   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 609      |\n",
      "|    ep_rew_mean        | -4.82    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6718     |\n",
      "|    iterations         | 1500     |\n",
      "|    time_elapsed       | 55       |\n",
      "|    total_timesteps    | 375000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.862    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1499     |\n",
      "|    policy_loss        | -0.0167  |\n",
      "|    value_loss         | 0.012    |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 595      |\n",
      "|    ep_rew_mean        | -4.87    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6718     |\n",
      "|    iterations         | 1600     |\n",
      "|    time_elapsed       | 59       |\n",
      "|    total_timesteps    | 400000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.771    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1599     |\n",
      "|    policy_loss        | 0.0145   |\n",
      "|    value_loss         | 0.0288   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 610      |\n",
      "|    ep_rew_mean        | -4.86    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6718     |\n",
      "|    iterations         | 1700     |\n",
      "|    time_elapsed       | 63       |\n",
      "|    total_timesteps    | 425000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.984    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1699     |\n",
      "|    policy_loss        | 0.00426  |\n",
      "|    value_loss         | 0.000934 |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 608      |\n",
      "|    ep_rew_mean        | -4.84    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6718     |\n",
      "|    iterations         | 1800     |\n",
      "|    time_elapsed       | 66       |\n",
      "|    total_timesteps    | 450000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1799     |\n",
      "|    policy_loss        | 0.0159   |\n",
      "|    value_loss         | 0.0104   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 616      |\n",
      "|    ep_rew_mean        | -4.81    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6717     |\n",
      "|    iterations         | 1900     |\n",
      "|    time_elapsed       | 70       |\n",
      "|    total_timesteps    | 475000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.923    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1899     |\n",
      "|    policy_loss        | 5.69e-06 |\n",
      "|    value_loss         | 0.00662  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=500000, episode_reward=-4.86 +/- 0.37\n",
      "Episode length: 628.80 +/- 133.01\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 629      |\n",
      "|    mean_reward        | -4.86    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 500000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.949    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 1999     |\n",
      "|    policy_loss        | -0.0162  |\n",
      "|    value_loss         | 0.00414  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 587      |\n",
      "|    ep_rew_mean     | -4.91    |\n",
      "| time/              |          |\n",
      "|    fps             | 5899     |\n",
      "|    iterations      | 2000     |\n",
      "|    time_elapsed    | 84       |\n",
      "|    total_timesteps | 500000   |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 588      |\n",
      "|    ep_rew_mean        | -4.95    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5933     |\n",
      "|    iterations         | 2100     |\n",
      "|    time_elapsed       | 88       |\n",
      "|    total_timesteps    | 525000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2099     |\n",
      "|    policy_loss        | 0.0152   |\n",
      "|    value_loss         | 0.00365  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 585      |\n",
      "|    ep_rew_mean        | -4.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 5965     |\n",
      "|    iterations         | 2200     |\n",
      "|    time_elapsed       | 92       |\n",
      "|    total_timesteps    | 550000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.789    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2199     |\n",
      "|    policy_loss        | -0.0459  |\n",
      "|    value_loss         | 0.0221   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 576      |\n",
      "|    ep_rew_mean        | -4.88    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5994     |\n",
      "|    iterations         | 2300     |\n",
      "|    time_elapsed       | 95       |\n",
      "|    total_timesteps    | 575000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.74     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2299     |\n",
      "|    policy_loss        | -0.0181  |\n",
      "|    value_loss         | 0.0712   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 593      |\n",
      "|    ep_rew_mean        | -4.86    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6020     |\n",
      "|    iterations         | 2400     |\n",
      "|    time_elapsed       | 99       |\n",
      "|    total_timesteps    | 600000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2399     |\n",
      "|    policy_loss        | 0.0122   |\n",
      "|    value_loss         | 0.01     |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 602      |\n",
      "|    ep_rew_mean        | -4.83    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6045     |\n",
      "|    iterations         | 2500     |\n",
      "|    time_elapsed       | 103      |\n",
      "|    total_timesteps    | 625000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.663    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2499     |\n",
      "|    policy_loss        | 0.00192  |\n",
      "|    value_loss         | 0.0425   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 601      |\n",
      "|    ep_rew_mean        | -4.83    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6068     |\n",
      "|    iterations         | 2600     |\n",
      "|    time_elapsed       | 107      |\n",
      "|    total_timesteps    | 650000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.799    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2599     |\n",
      "|    policy_loss        | -0.0236  |\n",
      "|    value_loss         | 0.0243   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 605      |\n",
      "|    ep_rew_mean        | -4.84    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6089     |\n",
      "|    iterations         | 2700     |\n",
      "|    time_elapsed       | 110      |\n",
      "|    total_timesteps    | 675000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.844    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2699     |\n",
      "|    policy_loss        | -0.0294  |\n",
      "|    value_loss         | 0.0132   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 596      |\n",
      "|    ep_rew_mean        | -4.93    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6109     |\n",
      "|    iterations         | 2800     |\n",
      "|    time_elapsed       | 114      |\n",
      "|    total_timesteps    | 700000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.92     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2799     |\n",
      "|    policy_loss        | 0.03     |\n",
      "|    value_loss         | 0.0127   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 618      |\n",
      "|    ep_rew_mean        | -4.87    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6128     |\n",
      "|    iterations         | 2900     |\n",
      "|    time_elapsed       | 118      |\n",
      "|    total_timesteps    | 725000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.946    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2899     |\n",
      "|    policy_loss        | 0.000148 |\n",
      "|    value_loss         | 0.00448  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 609      |\n",
      "|    ep_rew_mean        | -4.86    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6146     |\n",
      "|    iterations         | 3000     |\n",
      "|    time_elapsed       | 122      |\n",
      "|    total_timesteps    | 750000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.699    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 2999     |\n",
      "|    policy_loss        | -0.051   |\n",
      "|    value_loss         | 0.0508   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 604      |\n",
      "|    ep_rew_mean        | -4.89    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6162     |\n",
      "|    iterations         | 3100     |\n",
      "|    time_elapsed       | 125      |\n",
      "|    total_timesteps    | 775000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.798    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3099     |\n",
      "|    policy_loss        | -0.0251  |\n",
      "|    value_loss         | 0.0249   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 592      |\n",
      "|    ep_rew_mean        | -4.95    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6178     |\n",
      "|    iterations         | 3200     |\n",
      "|    time_elapsed       | 129      |\n",
      "|    total_timesteps    | 800000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.762    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3199     |\n",
      "|    policy_loss        | 0.0314   |\n",
      "|    value_loss         | 0.0177   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 585      |\n",
      "|    ep_rew_mean        | -4.88    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6193     |\n",
      "|    iterations         | 3300     |\n",
      "|    time_elapsed       | 133      |\n",
      "|    total_timesteps    | 825000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.986    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3299     |\n",
      "|    policy_loss        | 0.0112   |\n",
      "|    value_loss         | 0.00142  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 594      |\n",
      "|    ep_rew_mean        | -4.83    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6199     |\n",
      "|    iterations         | 3400     |\n",
      "|    time_elapsed       | 137      |\n",
      "|    total_timesteps    | 850000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.807    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3399     |\n",
      "|    policy_loss        | -0.0281  |\n",
      "|    value_loss         | 0.0297   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 592      |\n",
      "|    ep_rew_mean        | -4.87    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6206     |\n",
      "|    iterations         | 3500     |\n",
      "|    time_elapsed       | 140      |\n",
      "|    total_timesteps    | 875000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.847    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3499     |\n",
      "|    policy_loss        | -0.0118  |\n",
      "|    value_loss         | 0.0248   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 589      |\n",
      "|    ep_rew_mean        | -4.86    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6211     |\n",
      "|    iterations         | 3600     |\n",
      "|    time_elapsed       | 144      |\n",
      "|    total_timesteps    | 900000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.819    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3599     |\n",
      "|    policy_loss        | -0.0205  |\n",
      "|    value_loss         | 0.0284   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 594      |\n",
      "|    ep_rew_mean        | -4.86    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6217     |\n",
      "|    iterations         | 3700     |\n",
      "|    time_elapsed       | 148      |\n",
      "|    total_timesteps    | 925000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.787    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3699     |\n",
      "|    policy_loss        | -0.0413  |\n",
      "|    value_loss         | 0.0259   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 585      |\n",
      "|    ep_rew_mean        | -4.91    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6222     |\n",
      "|    iterations         | 3800     |\n",
      "|    time_elapsed       | 152      |\n",
      "|    total_timesteps    | 950000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3799     |\n",
      "|    policy_loss        | 0.00496  |\n",
      "|    value_loss         | 0.00742  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 598      |\n",
      "|    ep_rew_mean        | -4.92    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6227     |\n",
      "|    iterations         | 3900     |\n",
      "|    time_elapsed       | 156      |\n",
      "|    total_timesteps    | 975000   |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.977    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3899     |\n",
      "|    policy_loss        | -0.0164  |\n",
      "|    value_loss         | 0.00214  |\n",
      "------------------------------------\n",
      "Eval num_timesteps=1000000, episode_reward=-4.84 +/- 0.46\n",
      "Episode length: 577.99 +/- 107.69\n",
      "------------------------------------\n",
      "| eval/                 |          |\n",
      "|    mean_ep_length     | 578      |\n",
      "|    mean_reward        | -4.84    |\n",
      "| time/                 |          |\n",
      "|    total_timesteps    | 1000000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.908    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 3999     |\n",
      "|    policy_loss        | -0.00434 |\n",
      "|    value_loss         | 0.00636  |\n",
      "------------------------------------\n",
      "New best mean reward!\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 608      |\n",
      "|    ep_rew_mean     | -4.9     |\n",
      "| time/              |          |\n",
      "|    fps             | 5889     |\n",
      "|    iterations      | 4000     |\n",
      "|    time_elapsed    | 169      |\n",
      "|    total_timesteps | 1000000  |\n",
      "---------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 590      |\n",
      "|    ep_rew_mean        | -4.86    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5902     |\n",
      "|    iterations         | 4100     |\n",
      "|    time_elapsed       | 173      |\n",
      "|    total_timesteps    | 1025000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.938    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4099     |\n",
      "|    policy_loss        | 0.0146   |\n",
      "|    value_loss         | 0.00468  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 585      |\n",
      "|    ep_rew_mean        | -4.89    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5912     |\n",
      "|    iterations         | 4200     |\n",
      "|    time_elapsed       | 177      |\n",
      "|    total_timesteps    | 1050000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.91     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4199     |\n",
      "|    policy_loss        | 0.00947  |\n",
      "|    value_loss         | 0.00836  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 591      |\n",
      "|    ep_rew_mean        | -4.89    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5922     |\n",
      "|    iterations         | 4300     |\n",
      "|    time_elapsed       | 181      |\n",
      "|    total_timesteps    | 1075000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.82     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4299     |\n",
      "|    policy_loss        | -0.0429  |\n",
      "|    value_loss         | 0.0194   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 586      |\n",
      "|    ep_rew_mean        | -4.88    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5931     |\n",
      "|    iterations         | 4400     |\n",
      "|    time_elapsed       | 185      |\n",
      "|    total_timesteps    | 1100000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.912    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4399     |\n",
      "|    policy_loss        | 0.00561  |\n",
      "|    value_loss         | 0.00916  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 594      |\n",
      "|    ep_rew_mean        | -4.87    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5941     |\n",
      "|    iterations         | 4500     |\n",
      "|    time_elapsed       | 189      |\n",
      "|    total_timesteps    | 1125000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.939    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4499     |\n",
      "|    policy_loss        | 0.0185   |\n",
      "|    value_loss         | 0.00455  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 599      |\n",
      "|    ep_rew_mean        | -4.87    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5951     |\n",
      "|    iterations         | 4600     |\n",
      "|    time_elapsed       | 193      |\n",
      "|    total_timesteps    | 1150000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.757    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4599     |\n",
      "|    policy_loss        | -0.0819  |\n",
      "|    value_loss         | 0.0476   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 598      |\n",
      "|    ep_rew_mean        | -4.86    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5960     |\n",
      "|    iterations         | 4700     |\n",
      "|    time_elapsed       | 197      |\n",
      "|    total_timesteps    | 1175000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.722    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4699     |\n",
      "|    policy_loss        | 0.0167   |\n",
      "|    value_loss         | 0.0172   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 579      |\n",
      "|    ep_rew_mean        | -4.85    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5968     |\n",
      "|    iterations         | 4800     |\n",
      "|    time_elapsed       | 201      |\n",
      "|    total_timesteps    | 1200000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.753    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4799     |\n",
      "|    policy_loss        | 0.0225   |\n",
      "|    value_loss         | 0.0364   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 588      |\n",
      "|    ep_rew_mean        | -4.81    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5975     |\n",
      "|    iterations         | 4900     |\n",
      "|    time_elapsed       | 204      |\n",
      "|    total_timesteps    | 1225000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.842    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4899     |\n",
      "|    policy_loss        | 0.0027   |\n",
      "|    value_loss         | 0.0093   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 606      |\n",
      "|    ep_rew_mean        | -4.78    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5982     |\n",
      "|    iterations         | 5000     |\n",
      "|    time_elapsed       | 208      |\n",
      "|    total_timesteps    | 1250000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.794    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 4999     |\n",
      "|    policy_loss        | -0.0237  |\n",
      "|    value_loss         | 0.0249   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 631      |\n",
      "|    ep_rew_mean        | -4.79    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5989     |\n",
      "|    iterations         | 5100     |\n",
      "|    time_elapsed       | 212      |\n",
      "|    total_timesteps    | 1275000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.941    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5099     |\n",
      "|    policy_loss        | -0.0232  |\n",
      "|    value_loss         | 0.00564  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 615      |\n",
      "|    ep_rew_mean        | -4.87    |\n",
      "| time/                 |          |\n",
      "|    fps                | 5995     |\n",
      "|    iterations         | 5200     |\n",
      "|    time_elapsed       | 216      |\n",
      "|    total_timesteps    | 1300000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.832    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5199     |\n",
      "|    policy_loss        | -0.0271  |\n",
      "|    value_loss         | 0.0231   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 595      |\n",
      "|    ep_rew_mean        | -4.87    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6001     |\n",
      "|    iterations         | 5300     |\n",
      "|    time_elapsed       | 220      |\n",
      "|    total_timesteps    | 1325000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.865    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5299     |\n",
      "|    policy_loss        | -0.00132 |\n",
      "|    value_loss         | 0.0165   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 597      |\n",
      "|    ep_rew_mean        | -4.85    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6007     |\n",
      "|    iterations         | 5400     |\n",
      "|    time_elapsed       | 224      |\n",
      "|    total_timesteps    | 1350000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.88     |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5399     |\n",
      "|    policy_loss        | 0.0211   |\n",
      "|    value_loss         | 0.00997  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 600      |\n",
      "|    ep_rew_mean        | -4.85    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6013     |\n",
      "|    iterations         | 5500     |\n",
      "|    time_elapsed       | 228      |\n",
      "|    total_timesteps    | 1375000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.634    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5499     |\n",
      "|    policy_loss        | 0.0246   |\n",
      "|    value_loss         | 0.0561   |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 611      |\n",
      "|    ep_rew_mean        | -4.84    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6019     |\n",
      "|    iterations         | 5600     |\n",
      "|    time_elapsed       | 232      |\n",
      "|    total_timesteps    | 1400000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.876    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5599     |\n",
      "|    policy_loss        | 0.0168   |\n",
      "|    value_loss         | 0.00823  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 611      |\n",
      "|    ep_rew_mean        | -4.9     |\n",
      "| time/                 |          |\n",
      "|    fps                | 6026     |\n",
      "|    iterations         | 5700     |\n",
      "|    time_elapsed       | 236      |\n",
      "|    total_timesteps    | 1425000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.904    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5699     |\n",
      "|    policy_loss        | -0.00301 |\n",
      "|    value_loss         | 0.00967  |\n",
      "------------------------------------\n",
      "------------------------------------\n",
      "| rollout/              |          |\n",
      "|    ep_len_mean        | 611      |\n",
      "|    ep_rew_mean        | -4.89    |\n",
      "| time/                 |          |\n",
      "|    fps                | 6034     |\n",
      "|    iterations         | 5800     |\n",
      "|    time_elapsed       | 240      |\n",
      "|    total_timesteps    | 1450000  |\n",
      "| train/                |          |\n",
      "|    entropy_loss       | -2.08    |\n",
      "|    explained_variance | 0.715    |\n",
      "|    learning_rate      | 0.0007   |\n",
      "|    n_updates          | 5799     |\n",
      "|    policy_loss        | -0.0409  |\n",
      "|    value_loss         | 0.043    |\n",
      "------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 33\u001b[0m\n\u001b[1;32m      3\u001b[0m model \u001b[38;5;241m=\u001b[39m A2C(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMlpPolicy\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m      4\u001b[0m             vec_env, \n\u001b[1;32m      5\u001b[0m             learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     24\u001b[0m             device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     25\u001b[0m             _init_setup_model\u001b[38;5;241m=\u001b[39m_init_setup_model)\n\u001b[1;32m     27\u001b[0m eval_callback \u001b[38;5;241m=\u001b[39m EvalCallback(vec_env, \n\u001b[1;32m     28\u001b[0m                              best_model_save_path\u001b[38;5;241m=\u001b[39mLOGDIR, \n\u001b[1;32m     29\u001b[0m                              log_path\u001b[38;5;241m=\u001b[39mLOGDIR, \n\u001b[1;32m     30\u001b[0m                              eval_freq\u001b[38;5;241m=\u001b[39mEVAL_FREQ, \n\u001b[1;32m     31\u001b[0m                              n_eval_episodes\u001b[38;5;241m=\u001b[39mEVAL_EPISODES)\n\u001b[0;32m---> 33\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mNUM_TIMESTEPS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_callback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m model\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(LOGDIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_model\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/a2c/a2c.py:201\u001b[0m, in \u001b[0;36mA2C.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfA2C,\n\u001b[1;32m    194\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    199\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    200\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfA2C:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    208\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[0;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[0;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[1;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[0;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[1;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[1;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[0;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[1;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/vec_env/dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[0;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/stable_baselines3/common/monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[1;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/shimmy/openai_gym_compatibility.py:251\u001b[0m, in \u001b[0;36mGymV21CompatibilityV0.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[Any, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    243\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment.\u001b[39;00m\n\u001b[1;32m    244\u001b[0m \n\u001b[1;32m    245\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;124;03m        (observation, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m     obs, reward, done, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgym_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    253\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_mode \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/slimevolleygym/slimevolley.py:786\u001b[0m, in \u001b[0;36mSlimeVolleyEnv.step\u001b[0;34m(self, action, otherAction)\u001b[0m\n\u001b[1;32m    784\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m otherAction \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m: \u001b[38;5;66;03m# override baseline policy\u001b[39;00m\n\u001b[1;32m    785\u001b[0m   obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39magent_left\u001b[38;5;241m.\u001b[39mgetObservation()\n\u001b[0;32m--> 786\u001b[0m   otherAction \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    787\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39magent_left\u001b[38;5;241m.\u001b[39msetAction(otherAction)\n\u001b[1;32m    788\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;241m.\u001b[39magent_right\u001b[38;5;241m.\u001b[39msetAction(action) \u001b[38;5;66;03m# external agent is agent_right\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/slimevolleygym/slimevolley.py:529\u001b[0m, in \u001b[0;36mBaselinePolicy.predict\u001b[0;34m(self, obs)\u001b[0m\n\u001b[1;32m    527\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" take obs, update rnn state, return action \"\"\"\u001b[39;00m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setInputState(obs)\n\u001b[0;32m--> 529\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    530\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getAction()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.9/site-packages/slimevolleygym/slimevolley.py:509\u001b[0m, in \u001b[0;36mBaselinePolicy._forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    508\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprevOutputState \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputState\n\u001b[0;32m--> 509\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputState \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtanh(\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minputState\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vec_env = make_vec_env(slimevolleygym.SlimeVolleyEnv, n_envs=n_cpu, seed=SEED)\n",
    "\n",
    "model = A2C(\"MlpPolicy\", \n",
    "            vec_env, \n",
    "            learning_rate=learning_rate, \n",
    "            n_steps=n_steps, \n",
    "            gamma=gamma, \n",
    "            gae_lambda=gae_lambda, \n",
    "            ent_coef=ent_coef, \n",
    "            vf_coef=vf_coef, \n",
    "            max_grad_norm=max_grad_norm,\n",
    "            rms_prop_eps=rms_prop_eps, \n",
    "            use_rms_prop=use_rms_prop, \n",
    "            use_sde=use_sde, \n",
    "            sde_sample_freq=sde_sample_freq, \n",
    "            rollout_buffer_class=rollout_buffer_class, \n",
    "            rollout_buffer_kwargs=rollout_buffer_kwargs,\n",
    "            normalize_advantage=normalize_advantage, \n",
    "            stats_window_size=stats_window_size, \n",
    "            tensorboard_log=LOGDIR, \n",
    "            policy_kwargs=policy_kwargs, \n",
    "            verbose=verbose, \n",
    "            seed=SEED, \n",
    "            device=device,\n",
    "            _init_setup_model=_init_setup_model)\n",
    "\n",
    "eval_callback = EvalCallback(vec_env, \n",
    "                             best_model_save_path=LOGDIR, \n",
    "                             log_path=LOGDIR, \n",
    "                             eval_freq=EVAL_FREQ, \n",
    "                             n_eval_episodes=EVAL_EPISODES)\n",
    "\n",
    "model.learn(total_timesteps=NUM_TIMESTEPS, callback=eval_callback)\n",
    "\n",
    "model.save(os.path.join(LOGDIR, \"final_model\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
